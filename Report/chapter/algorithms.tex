\chapter{Benchmark algorithm}\label{cha:algorithms}
This part cover a possible applications for a \gls{GPGPU} study. The basic theory and motivation why they are suitable for benchmarking \gls{GPGPU} technologies is presented.

\section{Discrete Fourier Transform}
The Fourier transform is of use when analysing the spectrum of a continuous analogue signal. When applying transformation to a signal it is decomposed into the frequencies that makes it up. In digital signal analysis the \gls{DFT} is the counterpart of the Fourier transform for analogue signals. The \gls{DFT} converts a sequence of finite length into a list of coefficients of a finite combination of complex sinusoids. Given that the sequence is a sampled function from the time or spatial domain it is a conversion to the frequency domain. It is defined as 
\begin{equation}
X_k=\sum_{n=0}^{N-1}x(n)W_N^{kn}, k \in {[0, N-1]}
\end{equation}
where $W_N=e^{-\frac{i2{\pi}}{N}}$, commonly named the twiddle factor \cite{gentleman1966fast}.

The \gls{DFT} is used in many practical applications to perform Fourier analysis. It is a powerful mathematical tool that enables a perspective from another domain where difficult and complex problems becomes easier to analyse. Practically used in digital signal processing such as discrete samples of sound waves, radio signals or any continuous signal over a finite time interval. If used in image processing, the sampled sequence is pixels along a row or column. The \gls{DFT} takes input in complex numbers, and gives output in complex coefficients. In practical applications the input is usually real numbers.

\subsection{Fast Fourier Transform}\label{sec:algorithms:fft}
The problem with the \gls{DFT} is that the direct computation require $\mathcal{O}(n^n)$ complex multiplications and complex additions, which makes it computationally heavy and impractical in high throughput applications. The \gls{FFT} is one of the most common algorithms used to compute the \gls{DFT} of a sequence. A \gls{FFT} computes the transformation by factorizing the transformation matrix of the \gls{DFT} into a product of mostly zero factors. This reduces the order of computations to $\mathcal{O}(n\log{}n)$ complex multiplications and additions.

The FFT was made popular in 1965 \cite{cooley1965algorithm} by J.W Cooley and John Tukey. It found it is way into practical use at the same time, and meant a serious breakthrough in digital signal processing \cite{cooley1969fast, brigham1967fast}. However, the complete algorithm was not invented at the time. The history of the Cooley-Tukey \gls{FFT} algorithm can be traced back to around 1805 by work of the famous mathematician Carl Friedrich Gauss\cite{heideman1984gauss}. The algorithm is a divide-and-conquer algorithm that relies on recursively dividing the input into sub-blocks. Eventually the problem is small enough to be solved, and the sub-blocks are combined into the final result.

\section{Image processing}
Image processing consists of a wide range of domains. Earlier academic work with performance evaluation on the \gls{GPU} \cite{park2011design} tested four major domains and compared them with the \gls{CPU}. The domains were \gls{3D} shape reconstruction, feature extraction, image compression, and computational photography. Image processing is typically favourable on a \gls{GPU} since images are inherently a parallel structure.

Most image processing algorithms apply the same computation on a number of pixels, and that typically is a data parallel operation. Some algorithms can then be expected to have huge speed-up compared to an efficient \gls{CPU} implementation. A representative task is applying a simple image filter that gathers neighbouring pixel-values and compute a new value for a pixel. If done with respect to the underlying structure of the system, one can expect a speed-up near linear to the number of computational cores used. That is, a \gls{CPU} with four cores can theoretically expect a near four time speed-up compared to a single core. This extends to a \gls{GPU} so that a \gls{GPU} with n cores can expect a speed-up in the order of n in ideal cases. An example of this is a Gaussian blur (or smoothing) filter.

\section{Image compression}
The image compression standard \emph{JPEG2000} offers algorithms with parallelism but is very computationally and memory intensive. The standard aims to improve performance over JPEG, but also to add new features. The following sections are part of the JPEG2000 algorithm \cite{christopoulos2000jpeg2000}:
\begin{enumerate}
	\item Color Component transformation
	\item Tiling
	\item Wavelet transform
	\item Quantization
	\item Coding
\end{enumerate}

The computation heavy parts can be identified as the \gls{DWT} and the encoding engine uses \gls{EBCOT} Tier-1.

One difference between the older format \emph{JPEG} and the newer JPEG2000 is the use of \gls{DWT} instead of \gls{DCT}. In comparison to the \gls{DFT}, the \gls{DCT} operates solely on real values. \gls{DWT}s, on the other hand, uses another representation that allows for a time complexity of $\mathcal{O}(N)$.

\section{Linear algebra}
Linear algebra is central to both pure and applied mathematics. In scientific computing it is a highly relevant problem to solve dense linear systems efficiently. In the initial uses of GPUs in scientific computing, the graphics pipeline was successfully used for linear algebra through programmable vertex and pixel shaders \cite{kruger2003linear}. Methods and systems used later on for utilizing \gls{GPU}s have been shown efficient also in hybrid systems (multi-core \gls{CPU}s + \gls{GPU}s) \cite{tomov2010dense}. Linear algebra is highly suitable for \gls{GPU}s and with careful calibration it is possible to reach 80\%-90\% of the theoretical peak speed of large matrices \cite{volkov2008benchmarking}.

Common operations are vector addition, scalar multiplication, dot products, linear combinations, and matrix multiplication. Matrix multiplications have a high time complexity, $\mathcal{O}(N^3)$, which makes it a bottleneck in many algorithms. Matrix decomposition like LU, QR, and Cholesky decomposition are used very often and are subject for benchmark applications targeting \gls{GPU}s \cite{volkov2008benchmarking}.

\section{Sorting}
The sort operation is an important part of computer science and is a classic problem to work on. There exists several sorting techniques, and depending on problem and requirements a suitable algorithm is found by examining the attributes.

Sorting algorithms can be organized into two categories, data-driven and data-independent. The quicksort algorithm is a well known example of a data-driven sorting algorithm. It performs with time complexity $\mathcal{O}(n\log{n})$ on average, but have a time complexity of $\mathcal{O}(n^2)$ in the worst case. Another data-driven algorithm that does not have this problem is the heap sort algorithm, but it suffers from difficult data access patterns instead. Data-driven algorithms are not the easiest to parallelize since their behaviour is unknown and may cause bad load balancing among computational units.

The data independent algorithms is the algorithms that always perform the same process no matter what the data. This behaviour makes them suitable for implementation on multiple processors, and fixed sequences of instructions, where the moment in which data is synchronized and communication must occur are known in advance.

\subsection{Efficient sorting}
Bitonic sort have been used early on in the utilization of \gls{GPU}s for sorting. Even though it has the time complexity of $\mathcal{O}(n\log{n^2})$ it has been an easy way of doing a reasonably efficient sort on \gls{GPU}s. Other high performance sorting on \gls{GPU}s are often combinations of algorithms. Some examples of combined sort methods on GPUs are the bitonic merge sort, and a bucket sort that split the sequence into smaller sequences before each being sorted with a merge sort.

A popular algorithm for GPUs have been variants of radix sort which is a non-comparative integer sorting algorithm. Radix sorts can be described as being easy to implement and still as efficient as more sophisticated algorithms. Radix sort works by grouping the integer keys by the individual digit value in the same significant position and value.

\section{Criteria for Algorithm Selection}
A benchmarking application is sought that have the necessary complexity and relevance for both practical uses and the scientific community. The algorithm with enough complexity and challenges is the \gls{FFT}. Compared to the other presented algorithms the \gls{FFT} is more complex than the matrix operations and the regular sorting algorithms. The \gls{FFT} does not demand as much domain knowledge as the image compression algorithms, but it is still a very important algorithm for many applications.

The difficulties of working with multi-core systems are applied to \gls{GPU}s. What \gls{GPU}s are missing compared to multi-core \gls{CPU}s, is the power of working in sequential. Instead, \gls{GPU}s are excellent at fast context switching and hiding memory latencies. Most effort of working with \gls{GPU}s extends to supply tasks with enough parallelism, avoiding branching, and optimize memory access patterns. One important issue is also the host to device memory transfer-time. If the algorithm is much faster on the \gls{GPU}, a \gls{CPU} could still be faster if the host to device and back transfer is a large part of the total time.

By selecting an algorithm that have much scientific interest and history relevant comparisons can be made. It is sufficient to say that one can demand a reasonable performance by utilizing information sources showing implementations on \gls{GPU}s.