\chapter{Benchmark algorithm}\label{cha:algorithms}
This part cover possible applications for a \gls{GPGPU} study. The basic theory and motivation why they are suitable for benchmarking \gls{GPGPU} technologies is presented.% The \gls{FFT} algorithm is selected for the benchmarking application in this thesis work and will be in more detail in the next chapter.

\section{Discrete Fourier Transform}
The Fourier transform is of use when analysing the spectrum of a continuous analogue signal. When applying transformation to a signal it is decomposed into the frequencies that makes it up. In digital signal analysis the \gls{DFT} is the counterpart of the Fourier transform for analogue signals. The \gls{DFT} converts a sequence of finite length into a list of coefficients of a finite combination of complex sinusoids. Given that the sequence is a sampled function from the time or spatial domain it's a conversion to the frequency domain. It is defined as 
\begin{equation}
X_k=\sum_{n=0}^{N-1}x(n)W_N^{kn}, k \in {[0, N-1]}
\end{equation}
where $W_N=e^{-\frac{i2{\pi}}{N}}$, commonly named the twiddle factor\cite{gentleman1966fast}.

The \gls{DFT} is used in many practical applications to perform Fourier analysis. Its a powerful mathematical tool that enables a perspective from another domain where difficult and complex problems becomes easier to analyse. Practically used in digital signal processing, such as discrete samples of sound waves, radio signal or any continuous signal over a finite time interval. In image processing the sampled sequence can be pixels along a row or column. The \gls{DFT} takes input in complex numbers and outputs in complex coefficients. In practical applications the input is usually real numbers.

\subsection{Fast Fourier Transform}\label{sec:algorithms:fft}
The problem with the \gls{DFT} is that the direct computation require $\mathcal{O}(n^n)$ complex multiplications and complex additions, that makes it computationally heavy and impractical in high throughput applications. The \gls{FFT} is one of the most common algorithm used to compute the \gls{DFT} of a sequence. An \gls{FFT} computes the transformation by factorizing the transformation matrix of the \gls{DFT} into a product of mostly zero factors. This reduces the order of computations to $\mathcal{O}(n\log{}n)$ complex multiplications and additions.

The FFT was made popular in 1965\cite{cooley1965algorithm} by J.W Cooley and John Tukey and it found its way into practical use at the same time and meant a serious breakthrough in digital signal processing \cite{cooley1969fast, brigham1967fast}. However the complete algorithm was not invented at the time, the history of the Cooley-Tukey \gls{FFT} algorithm can be traced back to around 1805 by work of the famous mathematician Carl Friedrich Gauss\cite{heideman1984gauss}. The algorithm is a divide and conquer algorithm that relies on recursively dividing the input into sub-blocks and eventually the problem is small enough to be solved and the sub-blocks are combined into the final result.
\section{Image processing}
Image processing consists of a wide range of domains. Earlier academic work with performance evaluation on the \gls{GPU}\cite{park2011design} tested four major domains (\gls{3D} shape reconstruction, feature extraction, image compression and computational photography) and compared with the \gls{CPU}. Image processing is often by nature parallel and one can expect good results on a \gls{GPU}.

Most of image processing algorithms apply the same computation on a number of pixels and that is a typically data parallel operation. Some algorithms can then be expected to have huge speed up compared to an efficient \gls{CPU} implementation. A representative task is applying a simple image filter that gathers neighboring pixel-values and compute a new value for a pixel. If done with respect to the underlying structure of the system one can expect a speedup near linear to the number of computational cores used. That is a \gls{CPU} with four cores can theoretically expect a near four time speedup compared to a single core. This extends to a \gls{GPU} so a \gls{GPU} with n cores can in ideal cases expect a speedup in the order of n. An example of this is a Gaussian blur (or smoothing) filter.

\section{Image compression}
The image compression standard \emph{JPEG2000} offers algorithms with parallelism but is very computationally and memory intensive. The standard aims to improve performance over JPEG but also adding new features. The following sections are part of the JPEG2000 algorithm\cite{christopoulos2000jpeg2000}
\begin{enumerate}
	\item Color Component transformation
	\item Tiling
	\item Wavelet transform
	\item Quantization
	\item Coding
\end{enumerate}

The computation heavy parts can be identified as the \gls{DWT} and the encoding engine using \gls{EBCOT} Tier-1.

The important difference between the older format \emph{JPEG} compared to JPEG2000 is the use of \gls{DWT} instead of \gls{DCT}. In comparison to the \gls{DFT}, the \gls{DCT} operates solely on real values. \gls{DWT}s on the other hand uses another representation that allows for a time complexity of $\mathcal{O}(N)$.

\section{Linear algebra}
Linear algebra is central to both pure and applied mathematics. In scientific computing it's a highly relevant problem to solve dense linear systems efficiently. In the initial uses of GPUs in scientific computing, the graphics pipeline was successfully used for linear algebra through programmable vertex and pixel shaders \cite{kruger2003linear}. Methods and systems used later on for utilizing \gls{GPU}s have been shown efficient also in hybrid system (multi-core \gls{CPU}s + \gls{GPU}s)\cite{tomov2010dense}. Linear algebra is highly suitable for \gls{GPU}s and with careful calibration it is possible to reach 80\%-90\% of the theoretical peak speed of large matrices\cite{volkov2008benchmarking}.

Common operations are vector addition, scalar multiplication, dot products, linear combinations, and matrix multiplication. Matrix multiplications are of much interest since the high time complexity $\mathcal{O}(N^3)$ makes it a bottleneck in many algorithms. Matrix decomposition like LU, QR and Cholesky decomposition are used very often and are subject for benchmark applications targeting \gls{GPU}s\cite{volkov2008benchmarking}.

\section{Sorting}
The sort operation is an important part in computer science and have been a classic problem to work on. There exists several sorting techniques and depending on problem and requirements, a suitable algorithm is found by examining the attributes.

Sorting algorithms can be organized into two categories, data-driven and data-independent. The classic quicksort algorithm is probably the best known example of a data-driven sorting algorithm. It performs with time complexity $\mathcal{O}(n\log{n})$ on average but have a time complexity of $\mathcal{O}(n^2)$ in the worst case. Another data-driven algorithm that does not have this problem is heap sort but instead it suffers from difficult data access patterns. Data-driven algorithms are not the easiest to parallelize since the behaviour is unknown and may cause bad load balancing.

The other category are the algorithms that always perform the same process no matter what the data. This behaviour makes suitable for implementation on multiple processors, fixed sequences of instructions where the moment in which data is synchronized and communication must occur are known in advance.

\subsection{Efficient sorting}
Bitonic sort have been used early on in the utilization of \gls{GPU}s for sorting, even though it has the time complexity of $\mathcal{O}(n\log{n^2})$ it's been an easy way of doing reasonably efficient sort on \gls{GPU}s. Other high performance sorting on \gls{GPU}s are often combinations of algorithms. Examples of fast sorting algorithms on GPUs have used bucket sort or quicksort that first splits the list into sub list and then sort in parallel with merge sort or by using bitonic sort followed by merge sort.

A popular algorithm for GPUs have been variants of radix sort which is a non-comparative integer sorting algorithm. Radix sorts can be described as being easy to implement and still as efficient as more sophisticated algorithms. Radix sort works by grouping the integer keys by the individual digits value in the same significant position and value.

\section{Criteria for Algorithm Selection}
A benchmarking application is sought after that have the necessary complexity and relevance to both practical uses and the scientific community. The algorithm with enough complexity and challenges is the \gls{FFT}, compared to the other presented algorithms the \gls{FFT} are more complex than the matrix operations and the regular sorting algorithms. The \gls{FFT} does not demand as much domain knowledge as the image compression algorithms but its still a very important algorithm for many applications.

The difficulties working with multi-core systems are applied to \gls{GPU}s. What \gls{GPU}s are missing compared to multi-core \gls{CPU}s are the power of working in sequential, instead \gls{GPU}s are excellent at fast context switching and hiding memory latencies. Most effort of working with \gls{GPU}s extends to supply tasks with enough parallelism, avoiding branching and refine memory access patterns. One important issue is also the host to device memory transfer-time. If the algorithm is much faster on the \gls{GPU}, a \gls{CPU} could still be faster if the host to device and back transfer is a large part of the total time.

By selecting an algorithm that have much scientific interest and history; relevant comparisons can be made and it is sufficient to say that one can demand a reasonable performance by utilizing information sources using similar implementations on \gls{GPU}s.