\chapter{Evaluation}

\section{Results}

\newcommand{\plotwidth}{{\textwidth} / 2 + 110pt}
\newcommand{\AllPlacementOptions}{!htbp}

The results will be shown for both graphics cards, {\NVCARD} and \AMDCARD, where the technologies where applicable. The tested technologies are shown in table \ref{tab:platform-technologies}.

\begin{table}[\AllPlacementOptions]
	\centering
	\begin{tabular}{|l|l|}
		\hline
		Platform & Tested technology \\ \hline
		\multirow{3}{*}{\INTELCPU} & C/C++ \\
		{} & \OMP \\
		{} & \textit{\FFTW}\tablefootnote{Free software, available at \textit{http://fftw.org}.} \\ \hline	
		\multirow{5}{*}{\NVCARD} & \CU \\
		{} & \OCL \\
		{} & \DX \\
		{} & \GL \\
		{} & \textit{\CUFFT}\tablefootnote{NVIDIA developed library for use on CUDA-enabled graphics cards.} \\ \hline
		\multirow{4}{*}{\AMDCARD} & \OCL \\
		{} & \DX \\
		{} & \GL \\
		{} & \textit{\CLFFT}\tablefootnote{OpenCL FFT library, open source project.} \\ \hline
	\end{tabular}
	\caption{Technologies included in the experimental setup. External libraries {\FFTW}, {\CUFFT} and {\CLFFT} are included for comparisons.}
	\label{tab:platform-technologies}
\end{table}

The performance measure is time taken for a single forward transform using two buffers, an input and an output buffer. The implementation input size range is limited by the hardware (graphics card primary memory), however there are some unsolved issues near the upper limit on some technologies on the {\AMDCARD}.

{\CU} and the {\NVCARD} are the primary technology and platform and the other technologies are ported from {\CU}. To compare the implementation, external libraries are included and can be found italicised in the table \ref{tab:platform-technologies}. Note that the {\CLFFT} library failed to be measured in the same manner as the other GPU implementations, the times are measured at host.

The experiments are tuned on a few parameters, the number of threads per block and how large the tile dimensions are in the transpose kernel, see chapter \ref{cha:implementation} and table \ref{tab:threads-per-block}.

\subsection{Forward FFT}

The results for a single transform over a $2^{n}$-point sequence are shown in figure \ref{fig:gpu:overview} for the {\NVCARD}, {\AMDCARD} and {\INTELCPU}.

%1D TIME GTX
\begin{figure}[\AllPlacementOptions]
	\centering
	\subfloat[\NVCARD]{	
		\includestandalone[width=\plotwidth]{plots/gtx-overview}
	}
	\vfill
	\subfloat[\AMDCARD]{
		\includestandalone[width=\plotwidth]{plots/r260x-overview}
	}
	\caption{Overview of the results of a single forward transform. Lower is faster. The time of the {\CLFFT} library is measured at the host.}
	\label{fig:gpu:overview}
\end{figure}

The {\CU} implementation was the fastest on the {\NVCARD} over most sequences. For the {\AMDCARD} the {\OCL} implementation was the only who could run the whole test range. DirectCompute managed up to $2^{24}$ and OpenGL up to $2^{23}$. A normalized comparison using {\CU} and {\OCL} is shown in figure \ref{fig:gpu:implementation}.

\begin{figure}[\AllPlacementOptions]
	\centering
	\subfloat[\NVCARD\label{fig:gpu:implementation:gtx}]{	
		\includestandalone[width=\plotwidth]{plots/gtx-implementation}
	}
	\vfill
	\subfloat[\AMDCARD\label{fig:gpu:implementation:r260x}]{
		\includestandalone[width=\plotwidth]{plots/r260x-implementation}
	}	
	\caption{Performance relative {\CU} implementation in \ref{fig:gpu:implementation:gtx} and {\OCL} in \ref{fig:gpu:implementation:r260x}. Lower is better.}
	\label{fig:gpu:implementation}
\end{figure}

The experimental setup for the CPU involved low overhead and the short sequences could not be measured accurately, shown as $0{\micro}s$ in the figures. Results from comparing the sequential {\CPP} and multi-core {\OMP} implementation with {\CU} are shown in figure \ref{fig:gtx:cpu}. {\FFTW} was included and demonstrated how a optimized (per $n$-point length) sequential CPU implementation perform.

\begin{figure}[\AllPlacementOptions]
	\centering
	\includestandalone[width=\plotwidth]{plots/gtx-cpu}
	\caption{Performance relative {\CU} implementation on {\NVCARD} and {\INTELCPU}.}
	\label{fig:gtx:cpu}
\end{figure}

Results from comparing the implementations on the different graphics cards are shown in figure \ref{fig:gpu-comparison}. The results are normalized on the result of the tests on the {\NVCARD}.

\begin{figure}[\AllPlacementOptions]
	\centering
	\includestandalone[width=\plotwidth]{plots/gpu-comparison}
	\caption{Performance of respective implementation on the {\AMDCARD} and the {\NVCARD}.}
	\label{fig:gpu-comparison}
\end{figure}

{\DX}, {\GL} and {\OCL} was supported on both graphics cards, the results of normalizing the resulting times with the time of the {\OCL} implementation is shown in figure \ref{fig:gpu-comparison-tech}.

\begin{figure}[\AllPlacementOptions]
	\centering
	\includestandalone[width=\plotwidth]{plots/gpu-comparison-tech}
	\caption{Performance relative {\OCL} added from both cards.}
	\label{fig:gpu-comparison-tech}
\end{figure}

% 2D Figures

\newpage

\subsection{FFT 2D}

The equivalent test very done for 2D-data represented by an image of $m{\times}m$ size. The image contained three channels (red, green blue), the transformation was performed over one channel. Figure \ref{fig:gpu:overview-2d} shows an overview of the results of images seizes ranging from $2^{6}{\times}2^{6}$ to $2^{13}{\times}2^{13}$.

\begin{figure}[\AllPlacementOptions]
	\centering
	\subfloat[\NVCARD]{	
		\includestandalone[width=\plotwidth]{plots/gtx-overview-2d}
	}
	\vfill
	\subfloat[\AMDCARD]{
		\includestandalone[width=\plotwidth]{plots/r260x-overview-2d}
	}
	\caption{Overview of the results of measuring the time of a single 2D forward transform.}
	\label{fig:gpu:overview-2d}
\end{figure}

The implementations on the {\NVCARD} and {\AMDCARD} compared to {\CU} and {\OCL} is shown in figure \ref{fig:cpu:implementation-2d}. The {\GL} implementation failed at images larger then $2^{11}{\times}2^{11}$.

\begin{figure}[\AllPlacementOptions]
	\centering
	\subfloat[\NVCARD\label{fig:gpu:implementation-2d:gtx}]{	
		\includestandalone[width=\plotwidth]{plots/gtx-implementation}
	}
	\vfill
	\subfloat[\AMDCARD\label{fig:gpu:implementation-2d:r260x}]{
		\includestandalone[width=\plotwidth]{plots/r260x-implementation}
	}	
	\caption{2D performance relative {\CU} implementation in \ref{fig:gpu:implementation-2d:gtx} and {\OCL} in \ref{fig:gpu:implementation-2d:r260x}. Lower is better.}
	\label{fig:gpu:implementation-2d}
\end{figure}

The results of comparing the GPU and CPU handling of a 2D forward transform is shown in figure \ref{fig:gtx:cpu-2d}.

\begin{figure}[\AllPlacementOptions]
	\centering
	\includestandalone[width=\plotwidth]{plots/gtx-cpu-2d}
	\caption{Performance relative {\CU} implementation on {\NVCARD} and {\INTELCPU}.}
	\label{fig:gtx:cpu-2d}
\end{figure}

Comparison of the two cards are shown in figure \ref{fig:gpu-comparison-2d}.

\begin{figure}[\AllPlacementOptions]
	\centering
	\includestandalone[width=\plotwidth]{plots/gpu-comparison-2d}
	\caption{Performance of respective implementation on the {\AMDCARD} and the {\NVCARD}.}
	\label{fig:gpu-comparison-2d}
\end{figure}

{\DX}, {\GL} and {\OCL} was supported on both graphics cards, the results of normalizing the resulting times with the time of the {\OCL} implementation is shown in figure \ref{fig:gpu-comparison-tech-2d}.

\begin{figure}[\AllPlacementOptions]
	\centering
	\includestandalone[width=\plotwidth]{plots/gpu-comparison-tech-2d}
	\caption{Performance relative {\OCL} added from both cards.}
	\label{fig:gpu-comparison-tech-2d}
\end{figure}

\newpage

\section{Discussion}

The obvious competitors, based on other resarch-interest, is {\CU} and {\OCL}. The comparisons from earlier work have been focused primarily on the two \cite{Fang2011b}\cite{Karimi2010}\cite{Park2011}\cite{Su2012b}. Bringing {\DX} (or Direct3D Compute Shader) and {\GL} Compute Shader to table makes an interesting mix since the result from the experiment is that both are viable alternatives in terms of raw performance.

The most accurate and fair comparison with GPUs are when data is scaled up, the least amount of elements should be in the order of $2^{12}$. By not fully saturate the GPUs streaming multiprocessors there is no gain from moving from the CPU. One idea is to make sure that even if the sequences are short, they should be calculated in batches. The conclusions from running the benchmark application with small sequences should be taken lighter then the larger sequences. This is a flaw in the implementation to not handle more then one sequence at the time when sequences are short.

The implementation aimed at sequences of two-dimensional data was however successful at proving the strength of the GPU versus the CPU. The difference from the CPU to the GPU is 40 times faster when running a 2D FFT over large data. Compared to the multi-core {\OMP} solution the difference is still 15 times. Even the optimized {\FFTW} solution is 10 slower. As a side note, the {\CUFFT} is 36 times faster then {\FFTW} on large enough sequences, they do use the same strategy (build an execution plan based on current hardware and data size) but likely completely different algorithms.

The unsurprising result from the experiments are that {\CU} is the fastest technology on {\NVCARD}, but only with small a margin. What may or may not come as an surprise is the strength of the {\DX} implementation. Going head-to-head with {\CU} (only slightly slower) on the {\NVCARD} and performing equally or slightly faster then {\OCL}.

{\GL} is performing at the same level of {\DX} but only on the {\AMDCARD}. The exception is that the {\GL}-solution fails on the {\AMDCARD} at longer sequences otherwise working on the {\NVCARD}. The performance of the {\GL} tests are performance equal or better then {\OCL} in 1D but outperforming {\OCL} in 2D.

The biggest surprise is actually the {\OCL} implementation. Falling behind by a rather big margin, even behind on the \AMDCARD. Effort has been made to assure that the code does in fact run fair compared to the other technologies. The ratio for {\OCL} versus {\CU} on long sequences are about 1.6 and 1.8 times slower for 1D and 2D respectively on the {\NVCARD}. The figure \ref{fig:gpu-comparison-tech} and \ref{fig:gpu-comparison-tech-2d} shows that {\DX} is about 0.8 of the execution-time of {\OCL}. {\GL} beats the {\OCL} on shorter sequences about as effective as {\DX}, however does not handle longer sequences on the {\AMDCARD}. The one thing that goes in favor of {\OCL} is the fact that the implementation did scale without problem, all lengths of sequences was handles as expected. The figures \ref{fig:gpu:implementation:r260x} and \ref{fig:gpu:implementation-2d:r260x} shows that something happened with the other implementations, even {\CLFFT} had problem with the last sequence. {\GL} and {\DX} could not execute all sequences.

\subsection{Qualitative assessment}

When working with programming, raw performance is seldom the only requirement. This subsection will provide qualitative based assessments of the technologies used.

\subsubsection{Scalability of problems}



\subsubsection{Portability}

{\OCL} have a stated goal of being portable and open for many architecture enabling computations.

\subsubsection{Programmability}

{\CU} in the experience of this thesis, {\CU} was by far the least complicated to implement. The fewest lines of code needed to get started and few limitations compared to regular {\CPP}. The {\CU} community and online documentation is full of useful information, finding solutions to problem was relatively easy. The documentation at \textit{https://docs.nvidia.com/cuda/cuda-c-programming-guide/} provided guidance for most use cases.

{\OCL} implementation was not as straight forward as {\CU}. The biggest difference is the setup. Some differences in the setup:
\begin{itemize}
	\item Device selection, not needed actively in {\CU}
	\item Command queue or stream, created by default in {\CU}
	\item Create and build the kernel run-time instead of compile-time.
\end{itemize}
Both {\DX} and {\GL} follows this pattern, however both have inherently suffer from more graphics abstractions. Especially how memory buffers are created and handled are more prone to mistakes primarily because of the extra steps to actually create and use them in a compute shader.

The biggest issue with {\GL} is the way the device is selected, it is handled by the operating system (OS). It the case of running \textit{Windows 10}: the card have to be connected to a screen. Secondly, that screen needs to be selected as the primary screen. This issue is also a problem when using services like \textit{Windows Remote Desktop} (RDP). RDP enables the user to log in into a computer remotly, this works for the other technologies but not {\GL}.

\subsection{Method}

Was the benchmarking the right way to go? FFT?

\section{Conclusions}

\begin{itemize}
	\item Comparing overall performance on the {\NVCARD} (fastest first).
	\begin{enumerate}	
		\item {\CU}
		\item {\DX}
		\item {\GL}
		\item {\OCL}
	\end{enumerate}
	\item {\CU} compared to CPU implementations on long 1D sequences.
	\begin{itemize}
		\item {\OMP}: 13 times faster.
		\item {\CPP}: 18 times faster.
	\end{itemize}
	\item {\CU} compared to CPU implementations on large images.
	\begin{itemize}
		\item {\OMP}: 15 times faster.
		\item {\CPP}: 40 times faster.
	\end{itemize}
\end{itemize}

\section{Future work}

This thesis work leave room for expanding with more test applications and perhaps improve already implemented algorithm.

\subsection{Algorithm}

The implementation is much slower then the external libraries for the GPU, the room for improvements ought to be rather large. One can not expect to beat a mature and optimized library such as cuFFT, but one could at least expect a smaller difference in performance in some cases. Improved/further use of shared memory and explore a precomputed twiddle factor table would be interesting. Most important would probably be how the memory buffers are used in the context of data locality and stride.

\subsubsection{Test other algorithms}

The FFT algorithm is implemented in many practical applications, however the performance tests might give different results with other algorithms. The FFT is very easy parallelizable but put great demand on the memory by making large strides. It would be of interest to test algorithms, also highly parallelizable, but puts more strain on the use of arithmetic operations.

\subsection{The hardware}

\subsubsection{More technologies}

The graphic cards used in this thesis are at least one generation old compared to the current latest graphic cards. There would be interesting to see if the cards have the same differences in later series and to see how much have been improved over the generations. It is likely that the software drivers are differently optimized towards the newer graphic cards.

The DirectX 12 API was released Q4 2015 but this thesis only utilized the DirectX 11 API drivers. The release of \textit{Vulkan} which is a of low-overhead graphics and compute API comes with the premise much like DirectX 12 with high performance and more low level interaction. In a similar way AMDs Mantle is an alternative to Direct3D with the aim of reducing overhead. Most likely will the (new) hardware support the newer APIs more optimized during the next year.

\subsubsection{Graphics cards}

The {\NVCARD} have the \textit{Kepler} micro architecture. The model have been succeeded by booth the 700 and 900 GeForce series and the micro architecture have been followed by \textit{Maxwell} (2014). Both Kepler and Maxwell uses 28nm design. The next micro architecture is \textit{Pascal} and is due in 2016. Pascal will include 3D memory, HBM2 (High Bandwidth Memory), that will move onto the same package as the GPU and greatly improve memory bandwidth and total size. Pascal will use 16nm transistor design that will grant higher speed and energy effeciency.

The {\AMDCARD} have the GCN 1.1 (Graphics Core Next) micro architecture and have been succeeded by the Radeon Rx 300 Series and GCN 1.2. The latest graphic cards in the Rx 300 series include cards with HBM. The {\AMDCARD} is however not target towards the high-end consumer so it would be interesting to see the performance with a high-end AMD GPU.

\subsubsection{\INTELCPU}

The used {\INTELCPU} have four real cores but can utilize up to eight threads in hardware. Currently the trend is to utilize more cores per die when designing new CPUs. The release of Intel Core i7-6950X and i7-6900K targeting the high-end consumer market will have 10/20 and 8/16 cores / threads respectively. The i7-6950X is expected some time Q2 2016.

This cores will definitely challenge older GPUs, however it would make a interesting comparison with high-end consumer products by compare the newest multi-core CPU and GPU. This thesis was made with hardware from the same generation (released 2012-2013) and the development in parallel programming have progressed and matured even more since.