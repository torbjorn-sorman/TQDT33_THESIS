\chapter{Implementation}

The FFT algorithm has been implemented in C/C++, CUDA, OpenCL, DirectCompute and OpenGL on a GeForce GTX 670 and Radeon R7 260X graphics card and a Core i7 3770K 3.5GHz CPU.

\section{Application}

The implementation can be broken down into a few steps, see figure \ref{fig:algorithm-overview} for an simplified overview. The algorithm setup is platform-dependant but some steps are common; get platform and device information, allocate device buffers and upload data to device.
\begin{figure}
	\centering
	\includestandalone[width=\textwidth]{figures/overview}
	\caption{Overview of the events in the algorithm.}
	\label{fig:algorithm-overview}
\end{figure}

The next step is to calculate the specific FFT arguments for a $N$-point sequence for each kernel. The most important difference between devices and platforms are local memory capacity and thread and block configuration. Threads per block was selected for the best performance. See table \ref{tab:threads-per-block} for details.
\begin{table}
	\centering
	\includestandalone[width=\textwidth]{tables/threadsperblock}
	\caption{Shared memory size, threads and block configuration per device.}
	\label{tab:threads-per-block}
\end{table}

The implementation of a $N$-point radix-2 FFT algorithm have $\log_2 N$ stages with $N/2$ butterfly operations per stage. A butterfly operation is an addition, a subtraction, followed by a multiplication by a twiddle factor, showed in figure \ref{fig:butterfly}.
\begin{figure}[h]
	\centering
	\input{figures/butterfly}
	\caption{Butterfly operations}
	\label{fig:butterfly}
\end{figure}

The thread and block scheme was one butterfly per thread, so that a sequence of sixteen points require eight threads. Each platform was configured to a number of threads per block (see table \ref{tab:threads-per-block}) and any sequences twice as long as the threads per block configuration needed the algorithm to be split over several blocks. Example: if the threads per block limit is two, then four blocks would be needed for a sixteen point sequence.
\begin{figure}
	\input{figures/exampleflow}
	\caption{Example flow graph of a sixteen-point FFT using (stage 1 and 2) Cooley-Tukey algorithm and (stage 3 and 4) constant geometry algorithm. The solid box is the bit-reverse order output. Dotted boxes are seperate kernel launches, dashed boxes are data transfered to local memory before computing the remaining stages.}
	\label{fig:flowgraph-16}
\end{figure}

Thread synchronization is only available within a block. When the sequence or partial sequence fitted within a block all data was transfered to local memory before completing the last stages. If the sequence was larger and required more then one block the synchronization was handled by launching several kernels executed in sequence. The kernel launched for block wide synchronization is called the global kernel and the kernel for thread synchronization within a block is called the local kernel. The global kernel had an implementation of the Cooley-Tukey FFT algorithm and the local kernel had constant geometry (same indexing for every stage). The last stage outputs from shared memory to the bit reversed index of the complete sequence. See figure \ref{fig:flowgraph-16} where the sequence length is sixteen and the thread per block is set to two.

The indexing for the global kernel was calculated from the thread id and block id (threadIdx.x and blockIdx.x in CUDA) as seen in figure \ref{fig:code-global-index}. Input and output is done on the same index.
\begin{figure}
	\centering
	\lstset{language=C++}
	\begin{framed}
	\begin{lstlisting}
int tid     = blockIdx.x * blockDim.x + threadIdx.x,
    io_low  = tid + (tid & (0xFFFFFFFF << stages_left)),
    io_high = index1 + (N >> 1);
	\end{lstlisting}
	\end{framed}
	\caption{ CUDA example code showing index calculation for each stage in the global kernel, N is the total number of points. }
	\label{fig:code-global-index}
\end{figure}

Index calculation for the local kernel is done once for all stages, see figure \ref{fig:code-local-index}. These indexes are seperate from the indexing in the global memory. The global memory offset depends on threads per block (blockDim.x in CUDA) and block id.
\begin{figure}
	\centering
	\lstset{language=C++}
	\begin{framed}
	\begin{lstlisting}
int n_per_block = N / gridDim.x.
    in_low      = threadId.x.
    in_high     = threadId.x + (n_per_block >> 1).
    out_low     = threadId.x << 1.
    out_high    = out1 + 1;
	\end{lstlisting}
	\end{framed}
	\caption{ CUDA example code showing index calculation for points in shared memory for the CUDA local kernel. }
	\label{fig:code-local-index}
\end{figure}

The last operation after the last stage is to perform the bit-reverse indexing operation, this is done when writing from shared to global memory. The implementation of bit reversing is available as a single instruction on some platforms, if not available figure \ref{fig:code-bit-reverse} shows the code used. The bit reversed value had to be right shifted the number of zeroes leading the number in a 32-bit int. Example of the bit-reverse index operation: index 8 of a 16 point sequence is bit-reversed to 1, in bit-representation (binary) its 1000 to 0001. Index 8 of a 32 point sequence is bit-reversed to 2, correspons to 01000 to 00010. Figure \ref{fig:flowgraph-16} show the complete bit-reverse operation of all point in the last step after the last stage.

\begin{figure}[h]
	\centering
	\lstset{language=C++}
	\begin{framed}
	\begin{lstlisting}
x = (((x & 0xaaaaaaaa) >> 1) | ((x & 0x55555555) << 1));
x = (((x & 0xcccccccc) >> 2) | ((x & 0x33333333) << 2));
x = (((x & 0xf0f0f0f0) >> 4) | ((x & 0x0f0f0f0f) << 4));
x = (((x & 0xff00ff00) >> 8) | ((x & 0x00ff00ff) << 8));
return((x >> 16) | (x << 16));
	\end{lstlisting}
	\end{framed}
	\caption{ Kernel code producing a bit reversed unsigned integer where x is the input. }
	\label{fig:code-bit-reverse}
\end{figure}

The FFT algorithm for two dimensional sequences, such as images, is first a transform of each row (each row as a seperate sequence) and then a transform of each column. This GPU implementation does the first rowwise transformation and then transposes the whole image and repeat these to operations. An example is shown in figure \ref{fig:twodimentransform}.
\begin{figure}
	\centering
	\subfloat[Original image\label{image-1:lena}]{
		\includegraphics[keepaspectratio=true, scale=0.33]{images/lena.jpg}{}		
    }
    \hfill
    \subfloat[Magnitude representation\label{image-2:lena}]{
		\includegraphics[keepaspectratio=true, scale=0.33]{images/lena_transformed.jpg}{}
    }
	\caption{Original image \ref{image-1:lena} transformed and represented with a quadrant shifted magnitude visualization \ref{image-2:lena}. }
    \label{fig:twodimentransform}
\end{figure}