Automatically generated by Mendeley Desktop 1.15.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Stone2010,
author = {Stone, John E. and Gohara, David and Shi, Guochun},
file = {:C$\backslash$:/Users/torso/Documents/GitHub/TQDT33{\_}THESIS/Articles/OpenCL/OpenCL A Parallel Programming Standard for Heterogeneous Computing Systems.compressed.pdf:pdf},
title = {{OpenCL: A Parallel Programming Standard for Heterogeneous Computing Systems}},
year = {2010}
}
@article{Brodtkorb2013,
abstract = {Over the last decade, there has been a growing interest in the use of graphics processing units (GPUs) for non-graphics applications. From early academic proof-of-concept papers around the year 2000, the use of GPUs has now matured to a point where there are countless industrial applications. Together with the expanding use of GPUs, we have also seen a tremendous development in the programming languages and tools, and getting started programming GPUs has never been easier. However, whilst getting started with GPU programming can be simple, being able to fully utilize GPU hardware is an art that can take months or years to master. The aim of this article is to simplify this process, by giving an overview of current GPU programming strategies, profile-driven development, and an outlook to future trends. Â© 2012 Elsevier Inc. All rights reserved.},
author = {Brodtkorb, Andr{\'{e}} R. and Hagen, Trond R. and S{\ae}tra, Martin L.},
doi = {10.1016/j.jpdc.2012.04.003},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brodtkorb, Hagen, S{\ae}tra - 2013 - Graphics processing unit (GPU) programming strategies and trends in GPU computing.pdf:pdf},
isbn = {0743-7315},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {Debugging,Future trends,GPU computing,Hardware,Heterogeneous computing,Optimization,Profiling},
number = {1},
pages = {4--13},
title = {{Graphics processing unit (GPU) programming strategies and trends in GPU computing}},
volume = {73},
year = {2013}
}
@article{Smith2001,
author = {Smith, Randy K},
file = {:C$\backslash$:/Users/torso/Documents/GitHub/TQDT33{\_}THESIS/Articles/FFT/The+Design+and+Implementation+of+FFTW3.compressed.pdf:pdf},
issn = {1937-4771},
journal = {Southeastern Conference},
keywords = {adaptive software,cosine transform,fast fourier,fft,fourier transform,hartley transform,i,o tensor,transform},
number = {May 2001},
pages = {234--241},
title = {{the Design and Implementation of a}},
volume = {2},
year = {2001}
}
@article{Owens2006,
author = {Owens, John D and Luebke, David and Govindraju, Naga and Harris, Mark and Kruger, Jens and Lefohn, Aaron E and Purcell, Timothy J},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Owens et al. - 2006 - A Survey of General Purpose Computation on Graphics Hardware(2).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Owens et al. - 2006 - A Survey of General Purpose Computation on Graphics Hardware(3).pdf:pdf},
journal = {Computer Graphics Forum},
keywords = {1 computer graphics,2,2 software engineering,3,6 computer graphics,acm ccs,d,data-parallel computing,design tools and techniques,general-purpose computing on graphics,gpgpu,gpu,graphics hard-,hardware,hardware architecture,high-performance computing,hpc,i,methodology and tech-,niques,parallel computing,simd,stream computing,stream processing,ware},
number = {1},
pages = {80--113},
title = {{A Survey of General Purpose Computation on Graphics Hardware}},
url = {http://www.cs.virginia.edu/papers/ASurveyofGeneralPurposeComputationonGraphicsHardware.pdf},
volume = {26},
year = {2006}
}
@article{Du2012,
abstract = {In this work, we evaluate OpenCL as a programming tool for developing performance-portable applications for GPGPU. While the Khronos group developed OpenCL with programming portability in mind, performance is not necessarily portable. OpenCL has required performance-impacting initializations that do not exist in other languages such as CUDA. Understanding these implications allows us to provide a single library with decent performance on a variety of platforms. We choose triangular solver (TRSM) and matrix multiplication (GEMM) as representative level 3 BLAS routines to implement in OpenCL. We profile TRSM to get the time distribution of the OpenCL runtime system. We then provide tuned GEMM kernels for both the NVIDIA Tesla C2050 and ATI Radeon 5870, the latest GPUs offered by both companies. We explore the benefits of using the texture cache, the performance ramifications of copying data into images, discrepancies in the OpenCL and CUDA compilers' optimizations, and other issues that affect the performance. Experimental results show that nearly 50{\%} of peak performance can be obtained in GEMM on both GPUs in OpenCL. We also show that the performance of these kernels is not highly portable. Finally, we propose the use of auto-tuning to better explore these kernels' parameter space using search harness. ?? 2011 Elsevier B.V. All rights reserved.},
author = {Du, Peng and Weber, Rick and Luszczek, Piotr and Tomov, Stanimire and Peterson, Gregory and Dongarra, Jack},
doi = {10.1016/j.parco.2011.10.002},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Du et al. - 2012 - From CUDA to OpenCL Towards a performance-portable solution for multi-platform GPU programming.pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Du et al. - 2012 - From CUDA to OpenCL Towards a performance-portable solution for multi-platform GPU programming(2).pdf:pdf},
isbn = {0167-8191},
issn = {01678191},
journal = {Parallel Computing},
keywords = {Auto-tuning,Hardware accelerators,Portability},
number = {8},
pages = {391--407},
publisher = {Elsevier B.V.},
title = {{From CUDA to OpenCL: Towards a performance-portable solution for multi-platform GPU programming}},
url = {http://dx.doi.org/10.1016/j.parco.2011.10.002},
volume = {38},
year = {2012}
}
@article{Brigham1967,
abstract = {The fast Fourier transform (FFT), a computer algorithm that computes the discrete Fourier transform much faster than other algorithms, is explained. Examples and detailed procedures are provided to assist the reader in learning how to use the algorithm. The savings in computer time can be huge; for example, an N = 210-point transform can be computed with the FFT 100 times faster than with the use of a direct approach.},
author = {Brigham, E. O. and Morrow, R. E.},
doi = {10.1109/MSPEC.1967.5217220},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brigham, Morrow - 1967 - The fast Fourier transform(2).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brigham, Morrow - 1967 - The fast Fourier transform(3).pdf:pdf},
isbn = {013307496X},
issn = {0018-9235},
journal = {Spectrum, IEEE},
number = {12},
pages = {63 --70},
title = {{The fast Fourier transform}},
url = {http://ieeexplore.ieee.org/ielx5/6/5217195/05217220.pdf?tp={\&}arnumber=5217220{\&}isnumber=5217195$\backslash$nhttp://ieeexplore.ieee.org/stamp/stamp.jsp?tp={\&}arnumber=5217220},
volume = {4},
year = {1967}
}
@article{Park2011,
author = {Park, Ik and Singhal, Nitin and Lee, Mh and Cho, Sangdae and Kim, Cw},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Park et al. - 2011 - Design and performance evaluation of image processing algorithms on GPUs.pdf:pdf},
journal = {IEEE Transactions on Parallel and Distributed Systems},
number = {1},
pages = {91--104},
title = {{Design and performance evaluation of image processing algorithms on GPUs}},
volume = {22},
year = {2011}
}
@article{Cao2012,
author = {Cao, Yong},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao - 2012 - GPGPU Computing(2).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cao - 2012 - GPGPU Computing(3).pdf:pdf},
keywords = {cuda,gpu computing,opencl,parallel computing,stream},
number = {1978},
pages = {2026--2035},
title = {{GPGPU Computing}},
year = {2012}
}
@article{Gentleman1966,
author = {Gentleman, W M and Sande, G},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gentleman, Sande - 1966 - Fast Fourier Transforms for fun and profit.pdf:pdf},
journal = {Proceedings of the November 7-10, 1966, fall joint computer conference},
pages = {563--578},
title = {{Fast Fourier Transforms: for fun and profit}},
url = {papers2://publication/uuid/7065C1C0-089B-4DA8-8524-D5B62CB2B37A},
year = {1966}
}
@article{Govindaraju2008,
abstract = {We present novel algorithms for computing discrete Fourier transforms with high performance on GPUs. We present hierarchical, mixed radix FFT algorithms for both power-of-two and non-power-of-two sizes. Our hierarchical FFT algorithms efficiently exploit shared memory on GPUs using a Stockham formulation. We reduce the memory transpose overheads in hierarchical algorithms by combining the transposes into a block-based multi-FFT algorithm. For non-power-of-two sizes, we use a combination of mixed radix FFTs of small primes and Bluestein's algorithm. We use modular arithmetic in Bluestein's algorithm to improve the accuracy. We implemented our algorithms using the NVIDIA CUDA API and compared their performance with NVIDIA's CUFFT library and an optimized CPU-implementation (Intel's MKL) on a high-end quad-core CPU. On an NVIDIA GPU, we obtained performance of up to 300 GFlops, with typical performance improvements of 2-4times over CUFFT and 8-40times improvement over MKL for large sizes.},
author = {Govindaraju, N.K. Naga K and Lloyd, Brandon and Dotsenko, Yuri and Smith, Burton and Manferdelli, John},
doi = {10.1109/SC.2008.5213922},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Govindaraju et al. - 2008 - High performance discrete Fourier transforms on graphics processors(2).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Govindaraju et al. - 2008 - High performance discrete Fourier transforms on graphics processors(3).pdf:pdf},
isbn = {978-1-4244-2835-9},
journal = {Proceedings of the 2008 ACM/IEEE conference on Supercomputing},
number = {November},
pages = {2},
title = {{High performance discrete Fourier transforms on graphics processors}},
url = {http://portal.acm.org/citation.cfm?id=1413373},
year = {2008}
}
@article{Fatahalian2008,
abstract = {As the line between GPUs and CPUs begins to blur, itâs important to understand what makes GPUs tick. BY},
author = {Fatahalian, Kayvon and Houston, Mike},
doi = {10.1145/1400181.1400197},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fatahalian, Houston - 2008 - A closer look at GPUs.pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {50},
title = {{A closer look at GPUs}},
volume = {51},
year = {2008}
}
@article{Gregg2011,
abstract = {General purpose GPU Computing (GPGPU) has taken off in the past few years, with great promises for increased desktop processing power due to the large number of fast computing cores on high-end graphics cards. Many publications have demonstrated phenomenal performance and have reported speedups as much as 1000{\&}{\#}x00D7; over code running on multi-core CPUs. Other studies have claimed that well-tuned CPU code reduces the performance gap significantly. We demonstrate that this important discussion is missing a key aspect, specifically the question of where in the system data resides, and the overhead to move the data to where it will be used, and back again if necessary. We have benchmarked a broad set of GPU kernels on a number of platforms with different GPUs and our results show that when memory transfer times are included, it can easily take between 2 to 50{\&}{\#}x00D7; longer to run a kernel than the GPU processing time alone. Therefore, it is necessary to either include memory transfer overhead when reporting GPU performance, or to explain why this is not relevant for the application in question. We suggest a taxonomy for future CPU/GPU comparisons, and we argue that this is not only germane for reporting performance, but is important to heterogeneous scheduling research in general.},
author = {Gregg, Chris and Hazelwood, Kim},
doi = {10.1109/ISPASS.2011.5762730},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gregg, Hazelwood - 2011 - Where is the data Why you cannot debate CPU vs. GPU performance without the answer.pdf:pdf},
isbn = {9781612843681},
journal = {ISPASS 2011 - IEEE International Symposium on Performance Analysis of Systems and Software},
pages = {134--144},
title = {{Where is the data? Why you cannot debate CPU vs. GPU performance without the answer}},
year = {2011}
}
@article{Kruger2003,
abstract = {In this work, the emphasis is on the development of strategies to realize techniques of numerical computing on the graphics chip. In particular, the focus is on the acceleration of techniques for solving sets of algebraic equations as they occur in numerical simulation. We introduce a framework for the implementation of linear algebra operators on programmable graphics processors (GPUs), thus providing the building blocks for the design of more complex numerical algorithms. In particular, we propose a stream model for arithmetic operations on vectors and matrices that exploits the intrinsic parallelism and efficient communication on modern GPUs. Besides performance gains due to improved numerical computations, graphics algorithms benefit from this model in that the transfer of computation results to the graphics processor for display is avoided. We demonstrate the effectiveness of our approach by implementing direct solvers for sparse matrices, and by applying these solvers to multi-dimensional finite difference equations, i.e. the 2D wave equation and the incompressible Navier-Stokes equations.},
author = {Kr{\"{u}}ger, Jens and Westermann, R{\"{u}}diger},
doi = {10.1145/882262.882363},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kr{\"{u}}ger, Westermann - 2003 - Linear algebra operators for GPU implementation of numerical algorithms.pdf:pdf},
isbn = {1581137095},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {graphics hardware,numerical simulation},
number = {3},
pages = {908},
title = {{Linear algebra operators for GPU implementation of numerical algorithms}},
volume = {22},
year = {2003}
}
@article{Sintorn2008,
abstract = {This paper presents an algorithm for fast sorting of large lists using modern GPUs. The method achieves high speed by efficiently utilizing the parallelism of the GPU throughout the whole algorithm. Initially, GPU-based bucketsort or quicksort splits the list into enough sublists then to be sorted in parallel using merge-sort. The algorithm is of complexity n log n, and for lists of 8 M elements and using a single Geforce 8800 GTS-512, it is 2.5 times as fast as the bitonic sort algorithms, with standard complexity of n (log n)2, which for a long time was considered to be the fastest for GPU sorting. It is 6 times faster than single CPU quicksort, and 10{\%} faster than the recent GPU-based radix sort. Finally, the algorithm is further parallelized to utilize two graphics cards, resulting in yet another 1.8 times speedup. ?? 2008 Elsevier Inc. All rights reserved.},
author = {Sintorn, Erik and Assarsson, Ulf},
doi = {10.1016/j.jpdc.2008.05.012},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sintorn, Assarsson - 2008 - Fast parallel GPU-sorting using a hybrid algorithm.pdf:pdf},
isbn = {0743-7315},
issn = {07437315},
journal = {Journal of Parallel and Distributed Computing},
keywords = {GPU-algorithms,Parallelism,Sorting},
number = {10},
pages = {1381--1388},
title = {{Fast parallel GPU-sorting using a hybrid algorithm}},
volume = {68},
year = {2008}
}
@article{Gao2003a,
abstract = { Future high-end computers which promise very high performance require sophisticated program execution models and languages in order to deal with very high latencies across the memory hierarchy and to exploit massive parallelism. This paper presents our progress in an ongoing research toward this goal. Specifically we develop a suitable program execution model, a high-level programming notation which shields the application developer from the complexities of the architecture, and a compiler and runtime system based on the underlying models. In particular, we propose fine-grain multithreading and thread percolation as key components of our program execution model. We investigate implementing these models and systems on novel architectures such as the HTMT architecture and IBM's Blue Gene. Also, we report early performance prediction of thread percolation and its impact on execution time.},
author = {Gao, G.R. and Theobald, K.B. and Govindarajan, R. and Leung, C. and Hu, Ziang Hu Ziang and Wu, Haiping Wu Haiping and Lu, Jizhu Lu Jizhu and Cuvillo, J. Del and Jacquet, a. and Janot, V. and Sterling, T.L.},
doi = {10.1109/IPDPS.2003.1213378},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2003 - Programming models and system software for future high-end computing systems work-in-progress.pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gao et al. - 2003 - Programming models and system software for future high-end computing systems work-in-progress(2).pdf:pdf},
isbn = {0-7695-1926-1},
issn = {1530-2075},
journal = {Proceedings International Parallel and Distributed Processing Symposium},
number = {C},
title = {{Programming models and system software for future high-end computing systems: work-in-progress}},
volume = {00},
year = {2003}
}
@article{Cooley1969,
abstract = {The advent of the fast Fourier transform method has greatly extended our ability to implement Fourier methods on digital computers. A description of the alogorithm and its programming is given here and followed by a theorem relating its operands, the finite sample sequences, to the continuous functions they often are intended to approximate. An analysis of the error due to discrete sampling over finite ranges is given in terms of aliasing. Procedures for computing Fourier integrals, convolutions and lagged products are outlined.},
author = {Cooley, James W. and Lewis, Peter a. W. and Welch, Peter D.},
doi = {10.1109/TE.1969.4320436},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cooley, Lewis, Welch - 1969 - The Fast Fourier Transform and Its Applications.pdf:pdf},
isbn = {0133075052},
issn = {0018-9359},
journal = {IEEE Transactions on Education},
number = {1},
title = {{The Fast Fourier Transform and Its Applications}},
volume = {12},
year = {1969}
}
@article{Hosokawa2015,
author = {Hosokawa, Fumio and Sinkawa, Takao and Arai, Yoshihiro and Sannomiya, Takumi},
doi = {10.1016/j.ultramic.2015.06.018},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hosokawa et al. - 2015 - Benchmark test of accelerated multi-slice simulation by GPGPU.pdf:pdf},
issn = {03043991},
journal = {Ultramicroscopy},
keywords = {Benchmark,GPU,Image simulation,Multi-slice,STEM,TEM},
pages = {56--64},
publisher = {Elsevier},
title = {{Benchmark test of accelerated multi-slice simulation by GPGPU}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0304399115300061},
volume = {158},
year = {2015}
}
@article{Wonga,
author = {Wong, H. ; Papadopoulou, M.-M. ; Sadooghi-Alvandi, M. ; Moshovos, a. ; and {Dept. of Electr. {\&} Comput. Eng.}, Univ. of Toronto, Toronto, on, Canada},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong, Dept. of Electr. {\&} Comput. Eng. - Unknown - Demystifying GPU Microarchitecture throughnMicrobenchmarking(2).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wong, Dept. of Electr. {\&} Comput. Eng. - Unknown - Demystifying GPU Microarchitecture throughnMicrobenchmarking(3).pdf:pdf},
title = {{Demystifying GPU Microarchitecture through$\backslash$nMicrobenchmarking}},
url = {http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=5452013{\&}tag=1}
}
@article{Keckler2011,
abstract = {This article discusses the capabilities of state-of-the art GPU-based high-throughput computing systems and considers the challenges to scaling single-chip parallel-computing systems, highlighting high-impact areas that the computing research community can address. Nvidia Research is investigating an architecture for a heterogeneous high-performance computing system that seeks to address these challenges.},
author = {Keckler, Stephen W. and Dally, William J. and Khailany, Brucek and Garland, Michael and Glasco, David},
doi = {10.1109/MM.2011.89},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Keckler et al. - 2011 - GPUs and the future of parallel computing.pdf:pdf},
isbn = {0272-1732},
issn = {02721732},
journal = {IEEE Micro},
keywords = {GPU,Parallel-computer architecture,energy-efficient computing},
number = {5},
pages = {7--17},
title = {{GPUs and the future of parallel computing}},
volume = {31},
year = {2011}
}
@article{Vuduc2010c,
abstract = {This paper throws a small "wet blanket" on the hot topic of GPGPU acceleration, based on experience analyzing and tuning both multithreaded CPU and GPU implementations of three computations in scientific computing. These computations--(a) iterative sparse linear solvers; (b) sparse Cholesky factorization; and (c) the fast multipole method--exhibit complex behavior and vary in computational intensity and memory reference irregularity. In each case, algorithmic analysis and prior work might lead us to conclude that an idealized GPU can deliver better performance, but we find that for at least equal-effort CPU tuning and consideration of realistic workloads and calling-contexts, we can with two modern quad-core CPU sockets roughly match one or two GPUs in performance. Our conclusions are not intended to dampen interest in GPU acceleration; on the contrary, they should do the opposite: they partially illuminate the boundary between CPU and GPU performance, and ask architects to consider application contexts in the design of future coupled on-die CPU/GPU processors.},
author = {Vuduc, Richard and Chandramowlishwaran, Aparna and Choi, Jee and Guney, Murat and Shringarpure, Aashay},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vuduc et al. - 2010 - On the limits of GPU acceleration(3).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vuduc et al. - 2010 - On the limits of GPU acceleration(4).pdf:pdf},
issn = {1468-2044},
journal = {Proceedings of the 2nd USENIX conference on Hot topics in parallelism},
pages = {13},
pmid = {15695093},
title = {{On the limits of GPU acceleration}},
url = {http://dl.acm.org/citation.cfm?id=1863086.1863099},
year = {2010}
}
@article{Kwong2012,
author = {Kwong, Joyce and Goel, M.},
doi = {10.1109/DATE.2012.6176717},
file = {:C$\backslash$:/Users/torso/Documents/GitHub/TQDT33{\_}THESIS/Articles/FFT/A high performance split-radix FFT with constant geometry architecture.pdf:pdf},
isbn = {978-1-4577-2145-8},
issn = {15301591},
journal = {2012 Design, Automation {\&} Test in Europe Conference {\&} Exhibition (DATE)},
pages = {1537--1542},
title = {{A high performance split-radix FFT with constant geometry architecture}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6176717},
volume = {2},
year = {2012}
}
@article{Ye2010,
abstract = {Sorting is a kernel algorithm for a wide range of applications. In this paper, we present a new algorithm, GPU-Warpsort, to perform comparison-based parallel sort on Graphics Processing Units (GPUs). It mainly consists of a bitonic sort followed by a merge sort. Our algorithm achieves high performance by efficiently mapping the sorting tasks to GPU architectures. Firstly, we take advantage of the synchronous execution of threads in a warp to eliminate the barriers in bitonic sorting network. We also provide sufficient homogeneous parallel operations for all the threads within a warp to avoid branch divergence. Furthermore, we implement the merge sort efficiently by assigning each warp independent pairs of sequences to be merged and by exploiting totally coalesced global memory accesses to eliminate the bandwidth bottleneck. Our experimental results indicate that GPU-Warpsort works well on different kinds of input distributions, and it achieves up to 30{\%} higher performance than previous optimized comparison-based GPU sorting algorithm on input sequences with millions of elements.},
author = {Ye, Xiaochun and Fan, Dongrui and Lin, Wei and Yuan, Nan and Ienne, Paolo},
doi = {10.1109/IPDPS.2010.5470445},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ye et al. - 2010 - High performance comparison-based sorting algorithm on many-core GPUs.pdf:pdf},
isbn = {9781424464432},
issn = {1530-2075},
journal = {Proceedings of the 2010 IEEE International Symposium on Parallel and Distributed Processing, IPDPS 2010},
keywords = {Bitonic network,CUDA,GPU,Many-Core,Merge sort,Sorting algorithm},
title = {{High performance comparison-based sorting algorithm on many-core GPUs}},
year = {2010}
}
@article{Gardner2013a,
abstract = {The proliferation of heterogeneous computing systems has led to increased interest in parallel architectures and their associated programming models. One of the most promising models for heterogeneous computing is the accelerator model, and one of the most cost-effective, high-performance accelerators currently available is the general-purpose, graphics processing unit (GPU). Two similar programming environments have been proposed for GPUs: CUDA and OpenCL. While there are more lines of code already written in CUDA, OpenCL is an open standard that supports a broader. Hence, there is significant interest in automatic translation from CUDA to OpenCL. The contributions of this work are three-fold: (1) an extensive characterization of the subtle challenges of translation, (2) CU2CL (CUDA to OpenCL) - an implementation of a translator, and (3) an evaluation of CU2CL with respect to coverage of CUDA, translation performance, and performance of the translated applications. ?? 2013 Published by Elsevier B.V.},
author = {Gardner, Mark and Sathre, Paul and Feng, Wu Chun and Martinez, Gabriel},
doi = {10.1016/j.parco.2013.09.003},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gardner et al. - 2013 - Characterizing the challenges and evaluating the efficacy of a CUDA-to-OpenCL translator(2).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gardner et al. - 2013 - Characterizing the challenges and evaluating the efficacy of a CUDA-to-OpenCL translator(3).pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
keywords = {CUDA,GPU,OpenCL,Source-to-source translation},
number = {12},
pages = {769--786},
publisher = {Elsevier B.V.},
title = {{Characterizing the challenges and evaluating the efficacy of a CUDA-to-OpenCL translator}},
url = {http://dx.doi.org/10.1016/j.parco.2013.09.003},
volume = {39},
year = {2013}
}
@article{Chhugani2008,
abstract = {Sorting a list of input numbers is one of the most fundamental problems in the field of computer science in general and high-throughput database applications in particular. Although literature abounds with various flavors of sorting algorithms, different architectures call for customized implementations to achieve faster sorting times. This paper presents an efficient implementation and detailed analysis of MergeSort on current CPU architectures. Our SIMD implementation with 128-bit SSE is 3.3X faster than the scalar version. In addition, our algorithm performs an efficient multiway merge, and is not constrained by the memory bandwidth. Our multi-threaded, SIMD implementation sorts 64 million floating point numbers in less than0.5 seconds on a commodity 4-core Intel processor. This measured performance compares favorably with all previously published results. Additionally, the paper demonstrates performance scalability of the proposed sorting algorithm with respect to certain salient architectural features of modern chip multiprocessor (CMP) architectures, including SIMD width and core-count. Based on our analytical models of various architectural configurations, we see excellent scalability of our implementation with SIMD width scaling up to 16X wider than current SSE width of 128-bits, and CMP core-count scaling well beyond 32 cores. Cycle-accurate simulation of Intel's upcoming x86 many-core Larrabee architecture confirms scalability of our proposed algorithm.},
author = {Chhugani, Jatin and Nguyen, Anthony D. and Lee, Victor W. and Macy, William and Hagog, Mostafa and Chen, Yen-Kuang and Baransi, Akram and Kumar, Sanjeev and Dubey, Pradeep},
doi = {10.1145/1454159.1454171},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chhugani et al. - 2008 - Efficient implementation of sorting on multi-core SIMD CPU architecture.pdf:pdf},
isbn = {0000000000000},
issn = {2150-8097},
journal = {Proceedings of the VLDB Endowment},
number = {2},
pages = {1313--1324},
title = {{Efficient implementation of sorting on multi-core SIMD CPU architecture}},
url = {http://dl.acm.org/citation.cfm?id=1454171},
volume = {1},
year = {2008}
}
@article{Frigo1999,
author = {Frigo, Matteo and Frigo, Matteo},
file = {:C$\backslash$:/Users/torso/Documents/GitHub/TQDT33{\_}THESIS/Articles/FFT/A+Fast+Fourier+Transform+Compiler.compressed.pdf:pdf},
pages = {169--180},
title = {{A Fast Fourier Transform Compiler}},
volume = {34},
year = {1999}
}
@article{Tomov2010,
abstract = {Solving dense linear systems of equations is a fundamental problem in scientific computing. Numerical simulations involving complex systems represented in terms of unknown variables and relations between them often lead to linear systems of equations that must be solved as fast as possible. We describe current efforts toward the development of these critical solvers in the area of dense linear algebra (DLA) for multicore with GPU accelerators. We describe how to code/develop solvers to effectively use the high computing power available in these new and emerging hybrid architectures. The approach taken is based on hybridization techniques in the context of Cholesky, LU, and QR factorizations. We use a high-level parallel programming model and leverage existing software infrastructure, e.g. optimized BLAS for CPU and GPU, and LAPACK for sequential CPU processing. Included also are architecture and algorithm-specific optimizations for standard solvers as well as mixed-precision iterative refinement solvers. The new algorithms, depending on the hardware configuration and routine parameters, can lead to orders of magnitude acceleration when compared to the same algorithms on standard multicore architectures that do not contain GPU accelerators. The newly developed DLA solvers are integrated and freely available through the MAGMA library.},
author = {Tomov, Stanimire and Nath, Rajib and Ltaief, Hatem and Dongarra, Jack},
doi = {10.1109/IPDPSW.2010.5470941},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Tomov et al. - 2010 - Dense linear algebra solvers for multicore with GPU accelerators.pdf:pdf},
isbn = {9781424465347},
issn = {1878-3449},
journal = {Proceedings of the 2010 IEEE International Symposium on Parallel and Distributed Processing, Workshops and Phd Forum, IPDPSW 2010},
keywords = {Dense linear algebra solvers,GPU accelerators,Hybrid algorithms,MAGMA,Multicore},
pmid = {23958213},
title = {{Dense linear algebra solvers for multicore with GPU accelerators}},
year = {2010}
}
@article{Fang2011b,
abstract = {This paper presents a comprehensive performance comparison between CUDA and OpenCL. We have selected 16 benchmarks ranging from synthetic applications to real-world ones. We make an extensive analysis of the performance gaps taking into account programming models, ptimization strategies, architectural details, and underlying compilers. Our results show that, for most applications, CUDA performs at most 30$\backslash${\&}{\#}x025; better than OpenCL. We also show that this difference is due to unfair comparisons: in fact, OpenCL can achieve similar performance to CUDA under a fair comparison. Therefore, we define a fair comparison of the two types of applications, providing guidelines for more potential analyses. We also investigate OpenCL's portability by running the benchmarks on other prevailing platforms with minor modifications. Overall, we conclude that OpenCL's portability does not fundamentally affect its performance, and OpenCL can be a good alternative to CUDA.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Fang, Jianbin and Varbanescu, Ana Lucia and Sips, Henk},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fang, Varbanescu, Sips - 2011 - A comprehensive performance comparison of CUDA and OpenCL.pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fang, Varbanescu, Sips - 2011 - A comprehensive performance comparison of CUDA and OpenCL(2).pdf:pdf},
isbn = {9780769545103},
issn = {01903918},
journal = {Proceedings of the International Conference on Parallel Processing},
keywords = {CUDA,OpenCL,Performance Comparison},
pages = {216--225},
title = {{A comprehensive performance comparison of CUDA and OpenCL}},
year = {2011}
}
@article{Fatahalian,
author = {Fatahalian, B Y Kayvon and Houston, Mike and Wanders, a Gamer},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fatahalian, Houston, Wanders - Unknown - Look at GPUs.pdf:pdf},
title = {{Look at GPUs}}
}
@article{Hoare1962,
abstract = {A description is given of a new method of sorting in the random-access store of a computer. The method compares very favourably with other known methods in speed, in economy of storage, and in ease of programming. Certain refinements of the method, which may be useful in the optimization of inner loops, are described in the second part of the paper.},
author = {Hoare, Charles Antony Richard},
doi = {10.1093/comjnl/5.1.10},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoare - 1962 - Quicksort.pdf:pdf},
issn = {0010-4620},
journal = {The Computer Journal},
month = {jan},
number = {1},
pages = {10--16},
title = {{Quicksort}},
url = {http://comjnl.oupjournals.org/cgi/doi/10.1093/comjnl/5.1.10},
volume = {5},
year = {1962}
}
@article{Barik2014,
abstract = {There is growing interest in usingGPUs to accelerate general- purpose computation since they offer the potential of mas- sive parallelism with reduced energy consumption. This in- terest has been encouraged by the ubiquity of integrated processors that combine a GPU and CPU on the same die, lowering the cost of offloading work to the GPU. However, while the majority of effort has focused on GPU accelera- tion of regular applications, relatively little is known about the behavior of irregular applications on GPUs. These ap- plications are expected to perform poorly on GPUs without major software engineering effort. We present a compiler framework with support for C++ features that enables GPU acceleration of a wide range of C++ applications with min- imal changes. This framework, Concord, includes a low- cost, software SVM implementation that permits seamless sharing of pointer-containing data structures between the CPU and GPU. It also includes compiler optimizations to improve irregular application performance on GPUs. Us- ing Concord, we ran nine irregular C++ programs on two computer systems containing Intel 4th Generation Core pro- cessors. One system is an Ultrabook with an integrated HD Graphics 5000 GPU, and the other system is a desktop with an integrated HD Graphics 4600 GPU. The nine appli- cations are pointer-intensive and operate on irregular data structures such as trees and graphs; they include face detec- tion, BTree, single-source shortest path, soft-body physics simulation, and breadth-first search. Our results show that Concord acceleration using the GPU improves energy effi- ciency by up to 6.04Ã on the Ultrabook and 3.52Ã on the desktop over multicore-CPU execution.},
author = {Barik, Rajkishore and Lewis, Brian T},
doi = {10.1145/2581122.2544165},
file = {:C$\backslash$:/Users/torso/Documents/GitHub/TQDT33{\_}THESIS/Articles/IGP/Efficient+Mapping+of+Irregular+C+++Applications+to+Integrated+GPUs.compressed.pdf:pdf},
isbn = {9781450326704},
journal = {Proceedings of Annual {\ldots}},
keywords = {compiler optimization,en-,integrated gpu programming},
pages = {33--43},
title = {{Efficient Mapping of Irregular C++ Applications to Integrated GPUs}},
url = {http://dl.acm.org/citation.cfm?id=2544165},
year = {2014}
}
@article{Batcher1968,
abstract = {To achieve high throughput rates today's computers perform several operations simultaneously. Not only are I/O operations performed concurrently with computing, but also, in multiprocessors, several computing operations are done concurrently. A major problem in the design of such a computing system is the connecting together of the various parts of the system (the I/O devices, memories, processing units, etc.) in such a way that all the required data transfers can be accommodated. One common scheme is a high-speed bus which is time-shared by the various parts; speed of available hardware limits this scheme. Another scheme is a cross-bar switch or matrix; limiting factors here are the amount of hardware (an m Ã n matrix requires m Ã n cross-points) and the fan-in and fan-out of the hardware.},
author = {Batcher, K. E.},
doi = {10.1145/1468075.1468121},
file = {:C$\backslash$:/Users/torso/Documents/GitHub/TQDT33{\_}THESIS/Articles/Sorting/Sorting+networks+and+their+applications.compressed.pdf:pdf},
journal = {Proceedings of the April 30--May 2, 1968, spring joint computer conference on - AFIPS '68 (Spring)},
pages = {307},
title = {{Sorting networks and their applications}},
url = {http://dl.acm.org/citation.cfm?id=1468075.1468121},
year = {1968}
}
@article{Eirola2011,
abstract = {Modern graphics processors provide exceptional computa- tional power, but only for certain computational models. While they have revolutionized computation in many fields, compression has been largely unnaffected. This paper aims to explain the current issues and possibili- ties in GPGPU compression. This is done by a high level overview of the GPGPU computational model in the context of compression algorithms; along with a more in-depth analysis of how one would implement bzip2 on a GPGPU architecture.},
archivePrefix = {arXiv},
arxivId = {1109.2348},
author = {Eirola, Axel},
eprint = {1109.2348},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eirola - 2011 - Lossless data compression on GPGPU architectures.pdf:pdf},
title = {{Lossless data compression on GPGPU architectures}},
url = {http://arxiv.org/abs/1109.2348},
year = {2011}
}
@article{Ni2009e,
abstract = {Case study : Rolling Box Blur Applications Important point is that it provides a gauranteed feature set across all platforms and hardware unlike OpenCL},
author = {Ni, Tianyun},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ni - 2009 - Direct Compute Bring GPU Computing to the Mainstream Introduction(3).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ni - 2009 - Direct Compute Bring GPU Computing to the Mainstream Introduction(4).pdf:pdf},
keywords = {direct compute,directx},
number = {Nvidia GTC 09},
title = {{Direct Compute: Bring GPU Computing to the Mainstream Introduction}},
year = {2009}
}
@article{Su2012b,
abstract = {GPU (Graphics Processing Unit) has a great impact on computing field. To enhance the performance of computing systems, researchers and developers use the parallel computing architecture of GPU. On the other hand, to reduce the development time of new products, two programming models are included in GPU, which are OpenCL (Open Computing Language) and CUDA (Compute Unified Device Architecture). The benefit of involving the two programming models in GPU is that researchers and developers don't have to understand OpenGL, DirectX or other program design, but can use GPU through simple programming language. OpenCL is an open standard API, which has the advantage of cross-platform. CUDA is a parallel computer architecture developed by NVIDIA, which includes Runtime API and Driver API. Compared with OpenCL, CUDA is with better performance. In this paper, we used plenty of similar kernels to compare the computing performance of C, OpenCL and CUDA, the two kinds of API's on NVIDIA Quadro 4000 GPU. The experimental result showed that, the executive time of CUDA Driver API was 94.9{\%}{\~{}}99.0{\%} faster than that of C, while and the executive time of CUDA Driver API was 3.8{\%}{\~{}}5.4{\%} faster than that of OpenCL. Accordingly, the cross-platform characteristic of OpenCL did not affect the performance of GPU.},
author = {Su, Ching Lung and Chen, Po Yu and Lan, Chun Chieh and Huang, Long Sheng and Wu, Kuo Hsuan},
doi = {10.1109/APCCAS.2012.6419068},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Su et al. - 2012 - Overview and comparison of OpenCL and CUDA technology for GPGPU(2).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Su et al. - 2012 - Overview and comparison of OpenCL and CUDA technology for GPGPU(3).pdf:pdf},
isbn = {9781457717291},
journal = {IEEE Asia-Pacific Conference on Circuits and Systems, Proceedings, APCCAS},
pages = {448--451},
title = {{Overview and comparison of OpenCL and CUDA technology for GPGPU}},
year = {2012}
}
@article{Volkov2008,
author = {Volkov, Vasily and Demmel, James and Berkeley, U C},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Volkov, Demmel, Berkeley - 2008 - Benchmarking g GPUs to Tune Dense Linear Algebra.pdf:pdf},
isbn = {9781424428359},
number = {November},
title = {{Benchmarking g GPUs to Tune Dense Linear Algebra}},
year = {2008}
}
@article{Brodtkorb2010b,
abstract = {Node level heterogeneous architectures have become attractive during the last decade for several reasons: compared to traditional symmetric CPUs, they offer high peak performance and are energy and/or cost efficient. With the increase of fine-grained parallelism in high-performance computing, as well as the introduction of parallelism in workstations, there is an acute need for a good overview and understanding of these architectures. We give an overview of the state-of-the-art in heterogeneous computing, focusing on three commonly found architectures: the Cell Broadband Engine Architecture, graphics processing units (GPUs), and field programmable gate arrays (FPGAs). We present a review of hardware, available software tools, and an overview of state-of-the-art techniques and algorithms. Furthermore, we present a qualitative and quantitative comparison of the architectures, and give our view on the future of heterogeneous computing.},
author = {Brodtkorb, Andre R. and Dyken, Christopher and Hagen, Trond R. and Hjelmervik, Jon M. and Storaasli, Olaf O.},
doi = {10.3233/SPR-2009-0296},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brodtkorb et al. - 2010 - State-of-the-art in heterogeneous computing(2).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brodtkorb et al. - 2010 - State-of-the-art in heterogeneous computing(3).pdf:pdf},
isbn = {1058-9244},
issn = {10589244},
journal = {Scientific Programming},
keywords = {Energy and power consumption,Microprocessor performance,Parallel computer architecture,Power-efficient architectures,Stream or vector architectures},
number = {1},
pages = {1--33},
title = {{State-of-the-art in heterogeneous computing}},
volume = {18},
year = {2010}
}
@article{Satish2009,
author = {Satish, Nadathur and Harris, Mark and Garland, Michael},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Satish, Harris, Garland - 2009 - {\{}D{\}}esigning {\{}E{\}}fficient {\{}S{\}}oring {\{}A{\}}lgorithms for {\{}M{\}}anycore {\{}GPU{\}}s.pdf:pdf},
journal = {Proceedings of 23rd IEEE International Parallel and Distributed Processing Symposium},
pages = {1--10},
title = {{{\{}D{\}}esigning {\{}E{\}}fficient {\{}S{\}}oring {\{}A{\}}lgorithms for {\{}M{\}}anycore {\{}GPU{\}}s}},
year = {2009}
}
@article{Peng2000,
abstract = {Recently, OpenCL, a new open programming standard for GPGPU programming, has become available in addition toCUDA. OpenCL can support various compute devices due to its higher abstraction pro- gramming framework. Since there is a semantic gap between OpenCL and compute devices, the OpenCL C compiler plays important roles to exploit the potential of compute devices and therefore its capabil- ity should be clarified. In this paper, the performance of CUDA and OpenCL programs is quantitatively evaluated. First, several CUDA and OpenCL programs of almost the same computations are developed, and their performances are compared. Then, the main factors causing their performance differences is investigated. The evaluation results suggest that the performances of OpenCL programs are comparable with those of CUDA ones if the kernel codes are appropriately optimized by hand or by the compiler optimizations. This paper also discusses the differences between NVIDIA and AMD OpenCL implementations by comparing the performances of their GPUs for the same programs. The performance comparison shows that the compiler options of the OpenCL C compiler and the execution configuration parameters have to be optimized for each GPU to obtain its best performance. Therefore, automatic param- eter tuning is essential to enable a single OpenCL code to run efficiently on various GPUs.},
author = {Peng, Cheng and Chan, Andrew and Wang, Jianzhong and Komatsu, Kazuhiko and Sato, Katsuto and Arai, Yusuke and Koyama, Kentaro and Takizawa, Hiroyuki and Kobayashi, Hiroaki},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Komatsu et al. - 2010 - Evaluating Performance and Portability of OpenCL Programs.pdf:pdf},
journal = {Science And Technology},
number = {August 2015},
pages = {1663--1665},
title = {{Evaluating Performance and Portability of OpenCL Programs}},
url = {http://vecpar.fe.up.pt/2010/workshops-iWAPT/Komatsu-Sato-Arai-Koyama-Takizawa-Kobayashi.pdf},
volume = {2},
year = {2010}
}
@article{Nickolls2010,
author = {Nickolls, John and Dally, William J},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nickolls, Dally - 2010 - Gpu Computing Is At a Tipping Point , Becoming More Widely Used in Demanding Describes the Rapid Evolution of G.pdf:pdf},
journal = {Ieee Micro},
pages = {56--70},
pmid = {1000002922},
title = {{Gpu Computing Is At a Tipping Point , Becoming More Widely Used in Demanding Describes the Rapid Evolution of Gpu Architectures â From Graphics}},
year = {2010}
}
@article{Stone1966b,
abstract = {Description of FFT},
author = {Stone, H.S.},
doi = {10.1109/PGEC.1966.264428},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stone - 1966 - R66-50 An Algorithm for the Machine Calculation of Complex Fourier Series(2).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stone - 1966 - R66-50 An Algorithm for the Machine Calculation of Complex Fourier Series(3).pdf:pdf},
isbn = {00255718},
issn = {0367-7508},
journal = {IEEE Transactions on Electronic Computers},
number = {4},
pages = {297--301},
title = {{R66-50 An Algorithm for the Machine Calculation of Complex Fourier Series}},
volume = {EC-15},
year = {1966}
}
@article{Owens2008a,
abstract = {The graphics processing unit (GPU) has become an integral part of today's mainstream computing systems. Over the past six years, there has been a marked increase in the performance and capabilities of GPUs. The modern GPU is not only a powerful graphics engine but also a highly parallel programmable processor featuring peak arithmetic and memory bandwidth that substantially outpaces its CPU counterpart. The GPU's rapid increase in both programmability and capability has spawned a research community that has successfully mapped a broad range of computationally demanding, complex problems to the GPU. This effort in general-purpose computing on the GPU, also known as GPU computing, has positioned the GPU as a compelling alternative to traditional microprocessors in high-performance computer systems of the future. We describe the background, hardware, and programming model for GPU computing, summarize the state of the art in tools and techniques, and present four GPU computing successes in game physics and computational biophysics that deliver order-of-magnitude performance gains over optimized CPU applications.},
author = {Owens, J D and Houston, M and Luebke, D and Green, S and Stone, J E and Phillips, J C},
doi = {10.1109/JPROC.2008.917757},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Owens et al. - 2008 - GPU Computing.pdf:pdf},
isbn = {0018-9219},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {GPU computing,computational biophysics,computer graphic equipment,game physics,general-purpose computing,graphics engine,graphics processing unit,high-performance computer system,memory bandwidth,microcomputers,microprocessor,parallel computing,parallel programmable processor,parallel programming,peak arithmetic,programming model},
number = {5},
pages = {879--899},
pmid = {21776805},
title = {{GPU Computing}},
url = {http://ieeexplore.ieee.org/ielx5/5/4490117/04490127.pdf?tp={\&}arnumber=4490127{\&}isnumber=4490117$\backslash$nhttp://ieeexplore.ieee.org.ezproxyd.bham.ac.uk/ielx5/5/4490117/04490127.pdf?tp={\&}arnumber=4490127{\&}isnumber=4490117},
volume = {96},
year = {2008}
}
@article{Nickolls2010a,
abstract = {GPU computing is at a tipping point, becoming more widely used in demanding consumer applications and high-performance computing. This article describes the rapid evolution of GPU architectures-from graphics processors to massively parallel many-core multiprocessors, recent developments in GPU computing architectures, and how the enthusiastic adoption of CPU+GPU coprocessing is accelerating parallel applications.},
author = {Nickolls, John and Dally, William J.},
doi = {10.1109/MM.2010.41},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nickolls, Dally - 2010 - The GPU computing era.pdf:pdf},
isbn = {0272-1732},
issn = {02721732},
journal = {IEEE Micro},
keywords = {CUDA,Fermi GPU architecture,GPU computing,GPU coprocessing,Heterogeneous CPU+,NVIDIA.,Scalable parallel computing,Tesla GPU architecture},
number = {2},
pages = {56--69},
pmid = {1000002922},
title = {{The GPU computing era}},
volume = {30},
year = {2010}
}
@article{Garland2010e,
abstract = {Much has been written about the transition of commodity microprocessors from single-core to multicore chips, a trend most apparent in CPU processor families. Commodity PCs are now typically built with CPUs containing from two to eight cores, with even higher core counts on the ...},
author = {Garland, Michael and Kirk, David B.},
doi = {10.1145/1839676.1839694},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garland, Kirk - 2010 - Understanding throughput-oriented architectures.pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Garland, Kirk - 2010 - Understanding throughput-oriented architectures(2).pdf:pdf},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {11},
pages = {58},
title = {{Understanding throughput-oriented architectures}},
volume = {53},
year = {2010}
}
@article{Heideman1985,
abstract = {Histone methylation regulates chromatin structure, transcription, and epigenetic state of the cell. Histone methylation is dynamically regulated by histone methylases and demethylases such as LSD1 and JHDM1, which mediate demethylation of di- and monomethylated histones. It has been unclear whether demethylases exist that reverse lysine trimethylation. We show the JmjC domain-containing protein JMJD2A reversed trimethylated H3-K9/K36 to di- but not mono- or unmethylated products. Overexpression of JMJD2A but not a catalytically inactive mutant reduced H3-K9/K36 trimethylation levels in cultured cells. In contrast, RNAi depletion of the C. elegans JMJD2A homolog resulted in an increase in general H3-K9Me3 and localized H3-K36Me3 levels on meiotic chromosomes and triggered p53-dependent germline apoptosis. Additionally, other human JMJD2 subfamily members also functioned as trimethylation-specific demethylases, converting H3-K9Me3 to H3-K9Me2 and H3-K9Me1, respectively. Our finding that this family of demethylases generates different methylated states at the same lysine residue provides a mechanism for fine-tuning histone methylation.},
author = {Heideman, Michael T. and Johnson, Don H. and Burrus, C. Sidney},
doi = {10.1007/BF00348431},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Heideman, Johnson, Burrus - 1985 - Gauss and the history of the fast Fourier transform.pdf:pdf},
isbn = {0003-9519, 1432-0657},
issn = {00039519},
journal = {Archive for History of Exact Sciences},
number = {3},
pages = {265--277},
pmid = {16603238},
title = {{Gauss and the history of the fast Fourier transform}},
volume = {34},
year = {1985}
}
@article{Karimi2010,
abstract = {CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is an open standard that can be used to program CPUs, GPUs, and other devices from different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL promises a portable language for GPU programming, its generality may entail a performance penalty. In this paper, we use complex, near-identical kernels from a Quantum Monte Carlo application to compare the performance of CUDA and OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel to an OpenCL kernel involves minimal modifications. Making such a kernel compile with ATI's build tools involves more modifications. Our performance tests measure and compare data transfer times to and from the GPU, kernel execution times, and end-to-end application execution times for both CUDA and OpenCL.},
archivePrefix = {arXiv},
arxivId = {1005.2581},
author = {Karimi, Kamran and Dickson, Neil G. and Hamze, Firas},
doi = {10.1109/ICPP.2011.45},
eprint = {1005.2581},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Karimi, Dickson, Hamze - 2010 - A Performance Comparison of CUDA and OpenCL.pdf:pdf},
isbn = {978-1-4577-1336-1},
number = {1},
pages = {12},
title = {{A Performance Comparison of CUDA and OpenCL}},
url = {http://arxiv.org/abs/1005.2581},
year = {2010}
}
@article{Arndt2009,
abstract = {This is a book for the computationalist, whether a working programmer or anyone interested in methods of computation. The focus is on material that does not usually appear in textbooks on algorithms.},
author = {Arndt, Jorg},
doi = {10.1007/978-3-642-14764-7},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arndt - 2009 - Matters Computational.pdf:pdf},
isbn = {9783642147630},
journal = {Source},
pages = {978},
title = {{Matters Computational}},
url = {http://www.springerlink.com/index/10.1007/978-3-642-14764-7},
year = {2009}
}
@article{Wu2008,
author = {Wu, Enhua},
file = {:C$\backslash$:/Users/torso/Documents/GitHub/TQDT33{\_}THESIS/Articles/GPGPU/Emerging+technology+about+GPGPU.compressed.pdf:pdf},
isbn = {9781424423422},
journal = {Technology},
number = {October},
pages = {2008},
title = {{Emerging Technology ç©ä½ã®å½¢ç¶ãèªå¨ã«æã}},
year = {2008}
}
@article{Farooqui2014b,
author = {Farooqui, Naila and Schwan, Karsten and Yalamanchili, Sudhakar},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Farooqui, Schwan, Yalamanchili - 2014 - Efficient Instrumentation of GPGPU Applications Using Information Flow Analysis and Symbolic Exe.pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Farooqui, Schwan, Yalamanchili - 2014 - Efficient Instrumentation of GPGPU Applications Using Information Flow Analysis and Symbolic (2).pdf:pdf},
isbn = {9781450327664},
keywords = {cuda,gpgpu,opencl,rodinia},
title = {{Efficient Instrumentation of GPGPU Applications Using Information Flow Analysis and Symbolic Execution Categories and Subject Descriptors}},
year = {2014}
}
@article{Of2011,
author = {Of, Uture},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Of - 2011 - B Ioethical C Oncerns and the F Uture of P Lant G Enomics.pdf:pdf},
journal = {Genomics},
pages = {189--199},
title = {{B Ioethical C Oncerns and the F Uture of P Lant G Enomics}},
year = {2011}
}
@article{Kakimoto2012e,
author = {Kakimoto, Takeshi and Dohi, Keisuke and Shibata, Yuichiro and Oguri, Kiyoshi},
doi = {10.1145/2460216.2460229},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kakimoto et al. - 2012 - Performance comparison of GPU programming frameworks with the striped Smith-Waterman algorithm(3).pdf:pdf;:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kakimoto et al. - 2012 - Performance comparison of GPU programming frameworks with the striped Smith-Waterman algorithm(4).pdf:pdf},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
number = {5},
pages = {70},
title = {{Performance comparison of GPU programming frameworks with the striped Smith-Waterman algorithm}},
url = {http://dl.acm.org/citation.cfm?doid=2460216.2460229},
volume = {40},
year = {2012}
}
@article{Christopoulos2000,
abstract = {With the increasing use of multimedia technologies, image compression requires higher performance as well as new features. To address this need in the specific area of still image encoding, a new standard is currently being developed, the JPEG2000. It is not only intended to provide rate-distortion and subjective image quality performance superior to existing standards, but also to provide features and functionalities that current standards can either not address efficiently or in many cases cannot address at all. Lossless and lossy compression, embedded lossy to lossless coding, progressive transmission by pixel accuracy and by resolution, robustness to the presence of bit-errors and region-of-interest coding, are some representative features. It is interesting to note that JPEG2000 is being designed to address the requirements of a diversity of applications, e.g. Internet, color facsimile, printing, scanning, digital photography, remote sensing, mobile applications, medical imagery, digital library and E-commerce.},
author = {Christopoulos, Charilaos and Skodras, Athanassios and Ebrahimi, Touradj and Unit, Corporate},
file = {:C$\backslash$:/Users/torso/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Christopoulos et al. - 2000 - The {\{}JPEG2000{\}} Still Image Coding Systems An Overview.pdf:pdf},
journal = {IEEE Trans. Consumer Electronics},
keywords = {-- jpeg,JPEG,JPEG2000,color image coding,data compression,jpeg2000,source coding,subband coding,wavelet transform.},
number = {4},
pages = {1103--1127},
title = {{The {\{}JPEG2000{\}} Still Image Coding Systems: An Overview}},
volume = {46},
year = {2000}
}
