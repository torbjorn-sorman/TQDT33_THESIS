\chapter{Technologies}\label{cha:technologies}

\newcommand{\procwidth}{{\textwidth * 3 / 4}}

Five different multi-core technologies are used in this study. One is a proprietary parallel computing platform and \gls{API}, namely {\CU}. \textit{Compute Shaders} in {\GL} and {\DX} are parts of graphic programming languages but have a fairly general structure and allows for general computing. {\OCL} aims at any heterogeneous multi-core system and is used in this study solely on the \gls{GPU}. To compare with the \gls{CPU}, {\OMP} is included as an effective way to parallelize sequential {\CPP}-code.

\section{CUDA}

\gls{CUDA} is developed by NVIDIA and was released in 2006. {\CU} is an extension of the {\CPP} language and have its own compiler. {\CU} supports the functionality to execute kernels, and modify the graphic card RAM memory and the use of several optimized function libraries such as \textit{cuBLAS} ({\CU} implementation of \gls{BLAS}) and \textit{cuFFT} ({\CU} implementation of \gls{FFT}).

\glsreset{3D}

A program running on the \gls{GPU} is called a \gls{kernel}. The \gls{GPU} is referred to as the \textit{device} and the the \gls{CPU} is called the \textit{host}. To run a {\CU} \gls{kernel}, all that is needed is to declare the program with the function type specifier \code{\_\_global\_\_} and call it from the host with launch arguments, for other specifiers see table \ref{tab:cuda:func-types}. The \gls{kernel} execution call includes specifying the \gls{thread} organization. \Glspl{thread} are organized in \glspl{block}, that in turn are specified within a \emph{grid}. Both the \gls{block} and grid can be used as \gls{1D}, \gls{2D} or \gls{3D} to help the addressing in a program. These can be accessed within a \gls{kernel} by the structures \code{blockDim} and \code{gridDim}. \Gls{thread} and \gls{block} identification is done with \code{threadIdx} and \code{blockIdx}.

All limitations can be polled from the device and all devices are have a minimum feature support called \emph{Compute capability}. The compute capability aimed at in this thesis is $3.0$ and includes the NVIDIA \gls{GPU} models starting with \emph{GK} or later models (\emph{Tegra} and \emph{GM}).

{\CU} exposes intrinsic integer functions on the device and a variety of fast math functions, optimized single-precision operations, denoted with the suffix -\emph{f}. In the {\CU} example in figure \ref{lst:sample:global:cu} the trigonometric function \code{\_\_sincosf} is used to calculate both $\sin{\alpha}$ and $\cos{\alpha}$ in a single call.

\begin{table}
	%\includestandalone[width=\textwidth]{tables/cuda-function-types}
	\centering
	\input{tables/cuda-function-types}
	\caption{Table of function types in {\CU}.}
	\label{tab:cuda:func-types}
\end{table}

\begin{figure}
	\centering
	\fbox{\includestandalone[width=\procwidth]{code/sample/cu}}
	\caption{{\CU} global kernel}
	\label{lst:sample:global:cu}	
\end{figure}

\section{OpenCL}

{\OCL} is a framework and an open standard for writing programs that executes on multi-core platforms such as the \gls{CPU}, \gls{GPU} and \gls{FPGA} among other processors and hardware accelerators. {\OCL} uses a similar structure as {\CU}: The language is based on \emph{C99} when programming a device. The standard is supplied by the \emph{The Khronos Groups} and the implementation is supplied by the manufacturing company or device vendor such as AMD, INTEL, or NVIDIA.

{\OCL} views the system from a perspective where computing resources (\gls{CPU} or other accelerators) are a number of \emph{compute devices} attached to a host (a \gls{CPU}). The programs executed on a compute device is called a kernel. Programs in the {\OCL} language are intended to be compiled at run-time to preserve portability between implementations from various host devices.

The {\OCL} kernels are compiled by the host and then enqueued on a compute device. The kernel function accessible by the host to enqueue is specified with \code{\_\_kernel}. Data residing in global memory is specified in the parameter list by \code{\_\_global} and local memory have the specifier \code{\_\_local}. The {\CU} threads are in {\OCL} terminology called \emph{Work-items} and they are organized in \emph{Work-groups}.

\begin{figure}
	\centering
	\fbox{\includestandalone[width=\procwidth]{code/sample/ocl}}
	\caption{{\OCL} global kernel}
	\label{lst:sample:global:ocl}	
\end{figure}

Similarly to {\CU} the host application can poll the device for its capabilities and use some fast math function. The equivalent {\CU} kernel in figure \ref{lst:sample:global:cu} is implemented in {\OCL} in figure \ref{lst:sample:global:ocl} and displays small differences. The {\OCL} math function \code{sincos} is the equivalent of \code{\_\_sincosf}.

\section{DirectCompute}

Microsoft {\DX} is an \gls{API} that supports \gls{GPGPU} on Microsoft's Windows \gls{OS} (Vista, 7, 8, 10). {\DX} is part of the \emph{DirectX} collection of APIs. DirectX was created to support computer games development for the \emph{Windows 95} OS. The initial release of {\DX} was with DirectX 11 \gls{API}, and have similarities with both {\CU} and {\OCL}. {\DX} is designed and implemented with \gls{HLSL}. The program (and kernel equivalent) is called a \emph{compute shader}. The compute shader is not like the other types of shaders that are used in the graphic processing pipeline (like vertex or pixel shaders).

A difference from {\CU} and {\OCL} in implementing a compute shader compared to a kernel is the lack of C-like parameters: A \emph{constant buffer} is used instead, where each value is stored in a read-only data structure. The setup share similarities with {\OCL} and the program is compiled at run-time. The thread dimensions is built in as a constant value in the compute shader, and the block dimensions are specified at shader dispatch/execution.

As the code example demonstrated in figure \ref{lst:sample:global:dx} the shader body is similar to that of {\CU} and {\OCL}.

\begin{figure}
	\centering
	\fbox{\includestandalone[width=\procwidth]{code/sample/dx}}
	\caption{{\DX} global kernel}
	\label{lst:sample:global:dx}	
\end{figure}

\section{OpenGL}

\gls{OpenGL} share much of the same graphics inheritance as {\DX} but also provides a compute shader that breaks out of the graphics pipeline. The {\GL} is managed by the Khronos Group and was released in 1992. Analogous to \gls{HLSL}, {\GL} programs are implemented with \gls{GLSL}. The differences between the two are subtle, but include how arguments are passed and the use of specifiers.

Figure \ref{lst:sample:global:gl} show the {\GL} version of the global kernel.

\begin{figure}
	\centering
	\fbox{\includestandalone[width=\procwidth]{code/sample/gl}}
	\caption{{\GL} global kernel}
	\label{lst:sample:global:gl}	
\end{figure}

\section{OpenMP}

\gls{OpenMP} is an \gls{API} for multi-platform shared memory multiprocessing programming. It uses a set of compiler directives and library routines to implement multithreading. {\OMP} uses a master thread that \emph{forks} slave threads where work is divided among them. The threads runs concurrently and are allocated to different processors by the runtime environment. The parallel section of the code is marked with preprocessor directives (\code{\#pragma}) and when the threads are running the code they can access their respective id with the \code{omp\_get\_thread\_num()} call. When the section is processed the threads \emph{join} back into the master thread (with id $0$).

Figure \ref{lst:sample:global:omp} shows how the \emph{for-loop} section is parallelized by scheduling the workload evenly with the \code{static} keyword. The biggest difference in the {\OMP}-example is that the twiddle factor is computed in advance. Each thread will work on a consecutive span of the iterator variable.

\begin{figure}
	\centering
	\fbox{\includestandalone[width=\procwidth]{code/sample/omp}}%
	\caption{{\OMP} procedure completing one stage}%
	\label{lst:sample:global:omp}%
\end{figure}%

\section{External libraries}

External libraries were selected for reference values. {\FFTW} and {\CUFFT} were selected because they are frequently used in other papers. {\CLFFT} was selected by the assumption that it is the AMD equivalent of {\CUFFT} for AMDs graphic cards.

\subsubsection{FFTW}

\gls{FFTW} is a C subroutine library for computing the \gls{DFT}. {\FFTW} is a free software\cite{fftw2015} that have been available since 1997, and several papers have been published about the {\FFTW} \cite{frigo1999fast,frigo1998fftw,frigo2005design}. {\FFTW} supports a variety of algorithms and by estimating performance it builds a plan to execute the transform. The estimation can be done by either performance test of an extensive set of algorithms, or by a few known fast algorithms.

\subsubsection{cuFFT}

The library {\CUFFT} (NVIDIA CUDA Fast Fourier Transform product) \cite{nvidia2013userguide} is designed to provide high performance on NVIDIA \gls{GPU}s. {\CUFFT} uses algorithms based on the {\CTALG} and the Bluestein algorithm \cite{bluestein1970linear}.

\subsubsection{clFFT}

The library {\CLFFT}, found in \cite{githubclfft} is part of the open source \gls{ACL}\cite{amdacl}. According to an AMD blog post\cite{amd2015performance} the library performs at a similar level of {\CUFFT}\footnote{The performance tests was done using \emph{NVIDIA Tesla K40} for {\CUFFT} and \emph{AMD Firepro W9100} for {\CLFFT}.}.