\chapter{Theory}

This chapter will give an introduction to the FFT algorithm and a brief introduction of the Graphics Processing Unit (GPU).

\section{Graphics Processing Unit}

A GPU is traditionally specialized hardware for efficient manipulation of computer graphics and image processing. The inherent parallel structure of images and graphics makes them very efficient at some more general problems where parallelism can be exploited. The concept of General-purpose computing on graphics processing units (GPGPU) is applying a problem to the GPU platform instead of the CPU or multi-core system.

\subsection{Graphics hardware pipeline}

The traditional GPU can be described as a pipeline of a few linked steps. Every step receives input from the previous, processes the input and send it to the next. 

\subsection{GPGPU}

In the early days of GPGPU one had to rely on knowing a lot of graphics abstractions since the then available APIs was created with graphic processing in mind. The dominant APIs was OpenGL and DirectX (Direct3D).

\subsection{GPU vs CPU}

The GPU is build on a principle of more execution units instead of higher clock-frequency to improve performance. Comparing the two and the GPU performs a much higher floating point operations per second (FLOP) if running at optimal conditions. What the GPU sacrifice is the ability to run one task in sequential. The GPU relies much on using high memory bandwidth and fast context switching (run the next warp of threads) to compensate for lower frequency. The CPU is excellent at sequential tasks and uses branch prediction and ... among other finesses that is not used on the GPU.

\section{Fast Fourier Transform}

The Fast Fourier Transform is by far mostly associated with the Cooley-Tukey algorithm.

\subsection{FFT Cooley-Tukey}

The Cooley-Tukey algorithm is a devide and conquer algorithm that recursively breaks down a DFT of any composite size of $N = N_1{\cdot}N_2$.  

"
This is a divide and conquer algorithm that recursively breaks down a DFT of any composite size N = N1N2 into many smaller DFTs of sizes N1 and N2, along with O(N) multiplications by complex roots of unity traditionally called twiddle factors (after Gentleman and Sande, 1966[11]).
"

\subsubsection{FFT Constant Geometry}

Essentially the same algorithm but with some clever indexing the structure of the FFT do not changes through the stages and the indexing stays the same.

\subsection{FFT parallelism}

By looking at the FFT algorithm illustrated, it is easy to see how one can split operations over parallel tasks. Naturally one can start by selecting one thread per data input, however that would lead to unbalanced load as the second input is multiplied by the twiddle factor. By selecting one thread per butterfly operation each thread will share the same arithmetic workload.

\subsubsection{Problems parallelizing}

Memory latencies can be hidden by caches and fast context switching, however the distance in memory will matter. Large distances between data in the butterfly operations will make the memory the bottleneck. Coalesced memory read goes well with good context switching since the fewer memory request will be performed.

\subsection{FFT on GPU}

\begin{algorithm}
	\centering
	\begin{algorithmic}[1]
		\Procedure{GlobalKernel}{$data, bitmask, angle, stage, dist$}
            \State $tid \gets \Call{GlobalThreadId}{}$
            \newline
            \State // Calculate input offset          
            \State $low \gets tid + (tid \And bitmask)$
            \State $high \gets low + dist$            
            \newline
            \State // Calculate twiddle-factor
            \State $angle \gets angle \cdot ((tid \cdot 2^{stage}) \And \Call{ShiftLeft}{dist - 1, stage})$
            \State $\Call{Imag}{twiddleFactor} \gets \Call{Sin}{angle}$
            \State $\Call{Real}{twiddleFactor} \gets \Call{Cos}{angle}$
            \newline
            \State // Calculate butterfly-operations
            \State $temp \gets \Call{ComplexSub}{data_{low}, data_{high}}$
            \State $data_{low} \gets \Call{ComplexAdd}{data_{low}, data_{high}}$
            \State $data_{high} \gets \Call{ComplexMul}{temp, twiddleFactor}$
        \EndProcedure
	\end{algorithmic}
	\caption{Pseudo-code for the global kernel with input from the host.}
	\label{alg:device:global-kernel}
\end{algorithm}

\begin{algorithm}
	\centering
	\begin{algorithmic}[1]
		\Procedure{LocalKernel}{$input$, $output$, $angle$, $stepsLeft$, $leadingBits$, $scalar$, $blockRange$}
            \State let $shared$ be shared/local memory
            \State $offset \gets blockIdx.x \cdot blockDim.x \cdot 2$
            \State // Calculate input offset          
            \State $low \gets tid + (tid \And bitmask)$
            \State $high \gets low + dist$            
            \newline
            \State // Calculate twiddle-factor
            \State $angle \gets angle \cdot ((tid \cdot 2^{stage}) \And \Call{ShiftLeft}{dist - 1, stage})$
            \State $\Call{Imag}{twiddleFactor} \gets \Call{Sin}{angle}$
            \State $\Call{Real}{twiddleFactor} \gets \Call{Cos}{angle}$
            \newline
            \State // Calculate butterfly-operations
            \State $temp \gets \Call{ComplexSub}{data_{low}, data_{high}}$
            \State $data_{low} \gets \Call{ComplexAdd}{data_{low}, data_{high}}$
            \State $data_{high} \gets \Call{ComplexMul}{temp, twiddleFactor}$
        \EndProcedure
	\end{algorithmic}
	\caption{Pseudo-code for the local kernel with input from the host.}
	\label{alg:device:local-kernel}
\end{algorithm}